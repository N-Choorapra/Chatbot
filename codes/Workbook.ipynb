{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noogleai import NoogleAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcriber and param Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "\n",
    "def proxy_browser():\n",
    "    options = ChromeOptions()\n",
    "#     options.add_argument('--proxy-server={proxy}'.format(proxy=proxy))\n",
    "    options.add_argument('log-level=3')\n",
    "    options.add_argument(\"--window-size=1880x1020\")\n",
    "    ##for screen scrape\n",
    "    # options.add_argument(f'user-agent={userAgent}')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-infobars')\n",
    "    options.add_argument('--start-maximized')\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument('--disable-popup-blocking')\n",
    "\n",
    "\n",
    "    prefs = {\"profile.default_content_setting_values.notifications\" : 2,\n",
    "    \"webrtc.ip_handling_policy\" : \"disable_non_proxied_udp\",\n",
    "    \"webrtc.multiple_routes_enabled\": False,\n",
    "    \"webrtc.nonproxied_udp_enabled\" : False,\n",
    "    \"profile.default_content_setting_values.geolocation\" :2}\n",
    "    options.add_experimental_option(\"prefs\",prefs)\n",
    "    options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})\n",
    "    driver=Chrome(options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = proxy_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.service.is_connectable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthSkip to main contentWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate> cs > arXiv:2404.08801Help | Advanced SearchAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull textSearchopen searchGOopen navigation menuquick linksLogin\\nHelp Pages\\nAboutComputer Science > Machine LearningarXiv:2404.08801 (cs)[Submitted on 12 Apr 2024 (v1), last revised 16 Apr 2024 (this version, v2)]\\nTitle:Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length\\nAuthors:Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authors\\nView PDF\\nHTML (experimental)Abstract:The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: this https URLComments:\\n9 pages, 6 figures and 8 tablesSubjects:Machine Learning (cs.LG); Computation and Language (cs.CL)Cite as:\\narXiv:2404.08801 [cs.LG](orarXiv:2404.08801v2 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2404.08801Focus to learn morearXiv-issued DOI via DataCiteSubmission history From: Xuezhe Ma [view email][v1]Fri, 12 Apr 2024 20:28:14 UTC (568 KB)\\n[v2]Tue, 16 Apr 2024 07:27:58 UTC (572 KB)Full-text links:\\nAccess Paper:View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authorsView PDFHTML (experimental)TeX SourceOther Formats\\nview licenseCurrent browse context: cs.LG<\\xa0prev|next\\xa0>new|recent|2404Change to browse by:cs\\ncs.CLReferences & CitationsNASA ADSGoogle Scholar\\nSemantic Scholara\\nexport BibTeX citation\\nLoading...BibTeX formatted citation\\n√óloading...Data provided by:BookmarkBibliographic ToolsBibliographic and Citation ToolsBibliographic Explorer ToggleBibliographic Explorer (What is the Explorer?)Litmaps ToggleLitmaps (What is Litmaps?)scite.ai Togglescite Smart Citations (What are Smart Citations?)Code, Data, MediaCode, Data and Media Associated with this ArticleLinks to Code ToggleCatalyzeX Code Finder for Papers (What is CatalyzeX?)DagsHub ToggleDagsHub (What is DagsHub?)GotitPub ToggleGotit.pub (What is GotitPub?)Links to Code TogglePapers with Code (What is Papers with Code?)ScienceCast ToggleScienceCast (What is ScienceCast?)DemosDemosReplicate ToggleReplicate (What is Replicate?)Spaces ToggleHugging Face Spaces (What is Spaces?)Spaces ToggleTXYZ.AI (What is TXYZ.AI?)Related PapersRecommenders and Search ToolsLink to Influence FlowerInfluence Flower (What are Influence Flowers?)Connected Papers ToggleConnected Papers (What is Connected Papers?)Core recommender toggleCORE Recommender (What is CORE?)IArxiv recommender toggleIArxiv Recommender\\n(What is IArxiv?)Author\\nVenue\\nInstitution\\nTopicAbout arXivLabsarXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.Which authors of this paper are endorsers? |Disable MathJax (What is MathJax?)About\\nHelpcontact arXivClick here to contact arXivContactsubscribe to arXiv mailingsClick here to subscribeSubscribeCopyright\\nPrivacy PolicyWeb Accessibility AssistancearXiv Operational StatusGet status notifications viaemailor slack\", 'GitHub - horseee/Awesome-Efficient-LLM: A curated list for Efficient Large Language ModelsSkip to contentNavigation MenuToggle navigationSign inProductActionsAutomate any workflowPackagesHost and manage packagesSecurityFind and fix vulnerabilitiesCodespacesInstant dev environmentsCopilotWrite better code with AICode reviewManage code changesIssuesPlan and track workDiscussionsCollaborate outside of codeExploreAll featuresDocumentationGitHub SkillsBlogSolutionsForEnterpriseTeamsStartupsEducationBy SolutionCI/CD & AutomationDevOpsDevSecOpsResourcesLearning PathwaysWhite papers, Ebooks, WebinarsCustomer StoriesPartnersOpen SourceGitHub SponsorsFund open source developersThe ReadME ProjectGitHub community articlesRepositoriesTopicsTrendingCollectionsPricingSearch or jump to...Search code, repositories, users, issues, pull requests...SearchClearSearch syntax tipsProvide feedbackWe read every piece of feedback, and take your input very seriously.Include my email address so I can be contactedCancelSubmit feedbackSaved searchesUse saved searches to filter your results more quicklyNameQueryTo see all available qualifiers, see our documentation.CancelCreate saved searchSign inSign upYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.Dismiss alerthorseee/Awesome-Efficient-LLMPublicNotificationsFork61Star824A curated list for Efficient Large Language Models824stars61forksBranchesTagsActivityStarNotificationsCodeIssues\\n0Pull requests\\n1ActionsProjects\\n0SecurityInsightsAdditional navigation optionsCodeIssuesPull requestsActionsProjectsSecurityInsightshorseee/Awesome-Efficient-LLMThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History323 Commitsefficient_plmefficient_plmfiguresfiguresprojectprojectREADME.mdREADME.mdgenerate_item.pygenerate_item.pyView all filesRepository files navigationREADMEAwesome-Efficient-LLM\\nA curated list for Efficient Large Language Models:Knowledge Distillation\\nNetwork Pruning\\nQuantization\\nInference Acceleration\\nEfficient MOE\\nEfficient Architecture of LLM\\nText Compression\\nLow-Rank Decomposition\\nHardware/System\\nTuning\\nSurvey\\nLeaderboardüöÄ UpdatesSep 27, 2023: Add tagfor papers accepted at NeurIPS\\'23.\\nSep 6, 2023: Add a new subdirectory project/ to organize those projects that are designed for developing a lightweight LLM.\\nJuly 11, 2023:\\nIn light of the numerous publications that conducts experiments using PLMs (such as BERT, BART) currently, a new subdirectory efficient_plm/ is created to house papers that are applicable to PLMs but have yet to be verified for their effectiveness on LLMs (not implying that they are not suitable on LLM).üíÆ Contributing\\nIf you\\'d like to include your paper, or need to update any details such as conference information or code URLs, please feel free to submit a pull request. You can generate the required markdown format for each paper by filling in the information in generate_item.py and execute python generate_item.py. We warmly appreciate your contributions to this list. Alternatively, you can email me with the links to your paper and code, and I would add your paper to the list at my earliest convenience.\\nKnowledge DistillationTitle & Authors\\nIntroduction\\nLinksSpecializing Smaller Language Models towards Multi-Step ReasoningYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar KhotGithubPaperDistilling Script Knowledge from Large Language Models for Constrained Language PlanningSiyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing YangGithubPaperSCOTT: Self-Consistent Chain-of-Thought DistillationPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang RenPaperDISCO: Distilling Counterfactuals with Large Language ModelsZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle RichardsonGithubPaperI2D2: Inductive Knowledge Distillation with NeuroLogic and Self-ImitationChandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin ChoiGithubPaperProjectSymbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-StepLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin ChoiGithubPaperCan Language Models Teach? Teacher Explanations Improve Student Performance via Theory of MindSwarnadeep Saha, Peter Hase, and Mohit BansalGithubPaperDialogue Chain-of-Thought Distillation for Commonsense-aware Conversational AgentsHyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung YeoPaperPromptMix: A Class Boundary Augmentation Method for Large Language Model DistillationGaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. LaradjiGithubPaperTurning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative DataYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan LiGithubPaperDemocratizing Reasoning Ability: Tailored Learning from Large Language ModelZhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi ZhangGithubPaperGKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language ModelShicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Yang Yang, Hongyin Tang, Keqing He, Jiahao Liu, Jingang Wang, Shu Zhao, Peng Zhang, Jie TangGithubPaperDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model SizesCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas PfisterGithubPaperRetrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model CompressionJiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui YanPaperCache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language ModelsIlias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion AndroutsopoulosGithubPaperEfficiently Distilling LLMs for Edge ApplicationsAchintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih LeePaperLaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri AjiGithub paperKnowledge Distillation of Large Language ModelsYuxian Gu, Li Dong, Furu Wei, Minlie HuangGithubPaperTeaching Small Language Models to ReasonLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn.PaperLarge Language Model Distillation Doesn\\'t Need a TeacherAnanya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz BeltagyGithub paperThe False Promise of Imitating Proprietary LLMsArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn SongPaperImpossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and ParaphrasingJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin ChoiGithub paperPaD: Program-aided Distillation Specializes Large Models in ReasoningXuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen ZhouPaperRLCD: Reinforcement Learning from Contrast Distillation for Language Model AlignmentKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong TianPaperSci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QAYuhan Ma, Haiqi Jiang, Chenyou FanPaperUniversalNER: Targeted Distillation from Large Language Models for Open Named Entity RecognitionWenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung PoonGithubPaperProjectBaby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penaltyInar Timiryasov, Jean-Loup TastetGithubPaperDistillSpec: Improving Speculative Decoding via Knowledge DistillationYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran√ßois Kagy, Rishabh AgarwalPaperZephyr: Direct Distillation of LM AlignmentLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas WolfGithubPaperTowards the Law of Capacity Gap in Distilling Language ModelsChen Zhang, Dawei Song, Zheyu Ye, Yan GaoGithubPaperUnlock the Power: Competitive Distillation for Multi-Modal Large Language ModelsXinwei Li, Li Lin, Shuai Wang, Chen QianPaperMixed Distillation Helps Smaller Language Model Better ReasoningLi Chenglin, Chen Qianglong, Wang Caiyu, Zhang YinPaperDistilling Event Sequence Knowledge From Large Language ModelsSomin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker, Jian NiPaperKnowledge Distillation for Closed-Source Language ModelsHongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan, Ji ZhangPaperImproving Small Language Models\\' Mathematical Reasoning via Equation-of-Thought DistillationXunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping WangPaperScavenging Hyena: Distilling Transformers into Long Convolution ModelsTokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence LiangPaperDistiLLM: Towards Streamlined Distillation for Large Language ModelsJongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young YunGithubPaperLarge Language Model Meets Graph Neural Network in Knowledge DistillationShengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin ChenPaperUnmemorization in Large Language Models via Self-Distillation and Deliberate ImaginationYijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan VuliƒáGithubPaperTowards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMsNicolas Boizard, Kevin El-Haddad, C√©line Hudelot, Pierre ColomboGithub GithubPaperModelRevisiting Knowledge Distillation for Autoregressive Language ModelsQihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng TaoPaperPromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt TuningGyeongman Kim, Doohyuk Jang, Eunho YangPaperSelf-Distillation Bridges Distribution Gap in Language Model Fine-TuningZhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei ChenPaperWisdom of Committee: Distilling from Foundation Model to Specialized Application ModelZichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe ZhaoPaperDivide-or-Conquer? Which Part Should You Distill Your LLM?Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe ZhangPaperDistillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and DistillationPhuc Phan, Hieu Tran, Long PhanGithubPaperLeveraging Zero-Shot Prompting for Efficient Language Model DistillationLukas V√∂ge, Vincent Gurgul, Stefan LessmannPaperMetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction TasksLetian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo ShangGithubPaperModelGecko: Versatile Text Embeddings Distilled from Large Language ModelsJinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer et alPaperRethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language ModelsTaiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai WongPaperBlog-Eng Blog-‰∏≠Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language ModelsXiaoshu Chen, Sihang Zhou, Ke Liang, Xinwang LiuPaperCompressing Long Context for Enhancing RAG with AMR-based Concept DistillationKaize Shi, Xueyao Sun, Qing Li, Guandong XuPaperNetwork PruningTitle & Authors\\nIntroduction\\nLinksSparseGPT: Massive Language Models Can Be Accurately Pruned in One-ShotElias Frantar, Dan AlistarhGithub paperLLM-Pruner: On the Structural Pruning of Large Language ModelsXinyin Ma, Gongfan Fang, Xinchao WangGithub paperThe Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that MatterAjay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang WangGithubPaperFlash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured SparsityHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon SongGithubPaperA Simple and Effective Pruning Approach for Large Language ModelsMingjie Sun, Zhuang Liu, Anna Bair, J. Zico KolterGithubPaperSheared LLaMA: Accelerating Language Model Pre-training via Structured PruningMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi ChenGithubPaperPlug-and-Play: An Efficient Post-training Pruning Method for Large Language ModelsYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio CannistraciGithubPaperFluctuation-based Adaptive Structured Pruning for Large Language ModelsYongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao WangGithubPaperNASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language ModelsJongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young YunGithubPaperLoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-TuningMingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan ZhuangPaperPruning Large Language Models via Accuracy PredictorYupeng Ji, Yibo Cao, Jiucai LiuPaperCompressing LLMs: The Truth is Rarely Pure and Never SimpleAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei YangPaperJunk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through SparsityLu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang WangGithubPaperOutlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High SparsityLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei LiuGithubPaperCompresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language ModelsSong Guo, Jiahang Xu, Li Lyna Zhang, Mao YangGithubPaperSparse Finetuning for Inference Acceleration of Large Language ModelsEldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan AlistarhGithubPaperReLU Strikes Back: Exploiting Activation Sparsity in Large Language ModelsIman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad FarajtabarPaperThe Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context LearningTian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina DziugaitePaperOne-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language ModelsHang Shao, Bei Liu, Yanmin QianPaperLoRAShear: Efficient Large Language Model Structured Pruning and Knowledge RecoveryTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming LiangGithubPaperDivergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantizationBj√∂rn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias A√üenmacher, Kristian KerstingGithubPaperBeyond Size: How Gradients Shape Pruning Decisions in Large Language ModelsRocktim Jyoti Das, Liqun Ma, Zhiqiang ShenGithubPaperDynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMsYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong JiGithubPaperE-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M SparsityYun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui KangPaperPERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMsMax Zimmer, Megi Andoni, Christoph Spiegel, Sebastian PokuttaGithubPaperFast and Optimal Weight Update for Pruned Large Language ModelsVladim√≠r Bo≈æaGithubPaperPruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-TuningAdib Hasan, Ileana Rugina, Alex WangGithubPaperSliceGPT: Compress Large Language Models by Deleting Rows and ColumnsSaleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James HensmanGithubPaperAPT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and InferenceBowen Zhao, Hannaneh Hajishirzi, Qingqing CaoPaperReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMsZhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong SunPaperEverybody Prune Now: Structured Pruning of LLMs with only Forward PassesLucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet TalwalkarGithubPaperAssessing the Brittleness of Safety Alignment via Pruning and Low-Rank ModificationsBoyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia et alGithubPaperProjectNutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language ModelsShengrui Li, Xueting Han, Jing BaiPaperLearn To be Efficient: Build Structured Sparsity in Large Language ModelsHaizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul PrakashPaperShortened LLaMA: A Simple Depth Pruning for Large Language ModelsBo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu SongGithubPaperSLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer BlocksJiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon KimGithubPaperHiRE: High Recall Approximate Top-k Estimation for Efficient LLM InferenceYashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperLaCo: Large Language Model Pruning via Layer CollapseYifei Yang, Zouying Cao, Hai ZhaoPaperProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language ModelsChenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li et alGithubPaper[Model-7B] [Model-13B]EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMsSong Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong JiGithubPaperBESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity AllocationPeng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping LuoGithubPaperShortGPT: Layers in Large Language Models are More Redundant Than You ExpectXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng ChenPaperEfficient Pruning of Large Language Model with Adaptive Estimation FusionJun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi WangPaperDecoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under CompressionJunyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie et alGithubPaperProjectCompressing Large Language Models by Streamlining the Unimportant LayerXiaodong Chen, Yuxuan Hu, Jing ZhangPaperMultilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language BehindHongchuan Zeng, Hongshen Xu, Lu Chen, Kai YuGithubPaperAccelerating Inference in Large Language Models with a Unified Layer Skipping StrategyYijin Liu, Fandong Meng, Jie ZhouGithubPaperLoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language ModelsGuangyan Li, Yongqiang Tang, Wensheng ZhangPaperCATS: Contextually-Aware Thresholding for Sparsity in Large Language ModelsJe-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia MirhoseiniPaperLayer Skip: Enabling Early Exit Inference and Self-Speculative DecodingMostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer et alPaperEnabling High-Sparsity Foundational Llama Models with Efficient Pretraining and DeploymentAbhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, Sean Lie, Mark KurtzPaperDependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language ModelsZhiyu Guo, Hidetaka Kamigaito, Taro WanatnabePaperMixture-of-Depths: Dynamically allocating compute in transformer-based language modelsDavid Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam SantoroPaperQuantizationTitle & Authors\\nIntroduction\\nLinksGPTQ: Accurate Post-Training Quantization for Generative Pre-trained TransformersElias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan AlistarhGithubPaperSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language ModelsGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song HanGithubPaperQLoRA: Efficient Finetuning of Quantized LLMsTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke ZettlemoyerGithub PaperQuIP: 2-Bit Quantization of Large Language Models With GuaranteesJerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De SaXQGithubPaperMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer QuantizationJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo LeePaperQuantizable Transformers: Removing Outliers by Helping Attention Heads Do NothingYelysei Bondarenko, Markus Nagel, Tijmen BlankevoortGithub PaperLLM-FP4: 4-Bit Floating-Point Quantized TransformersShih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting ChengGithubPaperEnhancing Computation Efficiency in Large Language Models through Weight and Activation QuantizationJangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook ChoiPaperAgile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the EdgeXuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, Yanzhi WangPaperOmniQuant: Omnidirectionally Calibrated Quantization for Large Language ModelsWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping LuoGithubPaperAffineQuant: Affine Transformation Quantization for Large Language ModelsYuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong JiGithubPaperGPT-Zip: Deep Compression of Finetuned Large Language ModelsBerivan Isik, Hermann Kumbong, Wanyi Ning, Xiaozhe Yao, Sanmi Koyejo, Ce ZhangPaperWatermarking LLMs with Weight QuantizationLinyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng QiuGithubPaperAWQ: Activation-aware Weight Quantization for LLM Compression and AccelerationJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song HanGithubPaperRPTQ: Reorder-based Post-training Quantization for Large Language ModelsZhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe WuGithub PaperZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank CompensationZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong HePaperSqueezeLLM: Dense-and-Sparse Quantization Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt KeutzerGithubPaperOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scalingXiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong LiuPaperInteger or Floating Point? New Outlooks for Low-Bit Quantization on Large Language ModelsYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi XuPaperLLM-QAT: Data-Free Quantization Aware Training for Large Language ModelsZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas ChandraPaperSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan AlistarhGithubPaperOWQ: Lessons learned from activation outliers for weight quantization in large language modelsChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok ParkGithubPaperDo Emergent Abilities Exist in Quantized Large Language Models: An Empirical StudyPeiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong WenGithubPaperZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point FormatsXiaoxia Wu, Zhewei Yao, Yuxiong HePaperFPTQ: Fine-grained Post-Training Quantization for Large Language ModelsQingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen XiePaperQuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive AlgorithmKayhan Behdin, Ayan Acharya, Aman Gupta, Qingquan Song, Siyu Zhu, Sathiya Keerthi, Rahul MazumderGithubPaperNorm Tweaking: High-performance Low-bit Quantization of Large Language ModelsLiang Li, Qingyuan Li, Bo Zhang, Xiangxiang ChuPaperOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMsWenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao LvGithubPaperQA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language ModelsYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, Qi TianGithubPaperModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular QuantizersJunjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr KuleshovPaperPB-LLM: Partially Binarized Large Language ModelsYuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen DongGithubPaperDual Grained Quantization: Efficient Fine-Grained Quantization for LLMLuoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou, Hong ZhouPaperQFT: Quantized Full-parameter Tuning of LLMs with Affordable ResourcesZhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt KeutzerPaperQLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language ModelsJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan ZhuangPaperLoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language ModelsYixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo ZhaoPaperTEQ: Trainable Equivalent Transformation for Quantization of LLMsWenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao ShenGithubPaperBitNet: Scaling 1-bit Transformers for Large Language ModelsHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu WeiPaperAtom: Low-bit Quantization for Efficient and Accurate LLM ServingYilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris KasikciPaperAWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language ModelsBaisong Li, Xingwang Wang, Haixiao XuPaperAFPQ: Asymmetric Floating Point Quantization for LLMsYijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi XuGithubPaperA Speed Odyssey for Deployable Quantization of LLMsQingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen XiePaperLQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model FinetuningHan Guo, Philip Greengard, Eric P. Xing, Yoon KimGithubPaperEnabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous DequantizationJinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao DaiPaperSmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLMJiayi Pan, Chengcan Wang, Kaifu Zheng, Yangguang Li, Zhenyu Wang, Bin FengGithubPaperZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative TasksXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei YaoGithubPaperExtreme Compression of Large Language Models via Additive QuantizationVage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan AlistarhGithubPaperFP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-DesignHaojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon SongPaperKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache QuantizationColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir GholamiGithubPaperL4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQHyesung Jeon, Yulhwa Kim, Jae-joon KimPaperQuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice CodebooksAlbert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De SaGithubPaperBiLLM: Pushing the Limit of Post-Training Quantization for LLMsWei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan QiGithubPaperAccurate LoRA-Finetuning Quantization of LLMs via Information RetentionHaotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele MagnoGithubPaperApiQ: Finetuning of 2-Bit Quantized Large Language ModelBaohao Liao, Christof MonzPaperTowards Next-Level Post-Training Quantization of Hyper-Scale TransformersJunhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon JeonPaperEdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the EdgeXuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi WangGithubPaperBitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-DistillationDayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi XuGithubPaperWKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains MoreYuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang NiePaperDB-LLM: Accurate Dual-Binarization for Efficient LLMsHong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng TaoPaperOneBit: Towards Extremely Low-bit Large Language ModelsYuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang ChePaperBitDelta: Your Fine-Tune May Only Be Worth One BitJames Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle CaiGithubPaperAny-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMsYeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. LeePaperAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language ModelsZiyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao YuPaperGPTVQ: The Blessing of Dimensionality for LLM QuantizationMart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul WhatmoughGithubPaperA Comprehensive Evaluation of Quantization Strategies for Large Language ModelsRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi XiongPaperThe Era of 1-bit LLMs: All Large Language Models are in 1.58 BitsShuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu WeiPaperEvaluating Quantized Large Language ModelsShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu WangGithubPaperNo Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision QuantizationJune Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo LeePaperFlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor QuantizationYi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin PanPaperQAQ: Quality Adaptive Quantization for LLM KV CacheShichen Dong, Wen Cheng, Jiayu Qin, Wei WangGithubPaperWhat Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of PerturbationZhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui YanPaperFrameQuant: Flexible Low-Bit Quantization for TransformersHarshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas SinghPaperQuaRot: Outlier-Free 4-Bit Inference in Rotated LLMsSaleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James HensmanGithubPaperAccurate Block Quantization in LLMs with OutliersNikita Trukhanov, Ilya SoloveychikPaperCherry on Top: Parameter Heterogeneity and Quantization in Large Language ModelsWanyun Cui, Qianle WangPaperIncreased LLM Vulnerabilities from Fine-tuning and QuantizationDivyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth HarshangiPaperQuantization of Large Language Models with an Overdetermined BasisDaniil Merkulov, Daria Cherniuk, Alexander Rudikov, Ivan Oseledets, Ekaterina Muravleva, Aleksandr Mikhalev, Boris KashinPaperdecoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating PointsYi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping Cai, Yang Zhang, Shouda LiuGithubPaperLossless and Near-Lossless Compression for Foundation ModelsMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, Danny HarnikPaperHow Good Are Low-bit Quantized LLaMA3 Models? An Empirical StudyWei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele MagnoGithubPaperModelWhen Quantization Affects Confidence of Large Language Models?Irina Proskurina, Luc Brun, Guillaume Metzler, Julien VelcinGithubPaperQServe: W4A8KV4 Quantization and System Co-design for Efficient LLM ServingYujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song HanGithubPaperInference AccelerationTitle & Authors\\nIntroduction\\nLinksDeja Vu: Contextual Sparsity for Efficient LLMs at Inference TimeZichang Liu, Jue WANG, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi ChenGithubPaperScissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test TimeZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali ShrivastavaPaperDynamic Context Pruning for Efficient and Interpretable Autoregressive TransformersSotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas HofmannPaperH2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language ModelsZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang, Beidi ChenGithubPaperLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperFast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel DecodingSangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young YunGithubPaperCompressing Context to Enhance Inference Efficiency of Large Language ModelsYucheng Li, Bo Dong, Chenghua Lin, Frank GuerinGithubPaperConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models InferenceZiqian Zeng, Yihuai Hong, Hongliang Dai, Huiping Zhuang, Cen ChenPaperAccelerating LLM Inference with Staged Speculative DecodingBenjamin Spector, Chris RePaperTCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost ReductionJunyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming QianPaperInference with Reference: Lossless Acceleration of Large Language ModelsNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, Furu WeiGithubpaperSpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree VerificationXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao JiaGithubpaperSkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM InferenceLuciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, Subhabrata MukherjeePaperSkeleton-of-Thought: Large Language Models Can Do Parallel DecodingXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu WangPaperDraft & Verify: Lossless Large Language Model Acceleration via Self-Speculative DecodingJun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad MehrotraGithubPaperEfficient Streaming Language Models with Attention SinksGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike LewisGithubPaper(Dynamic) Prompting might be all you need to repair Compressed LLMsDuc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang WangPaperModel Tells You What to Discard: Adaptive KV Cache Compression for LLMsSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng GaoPaperLarge Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient ReasoningMurong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu YaoGithubPaperLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt CompressionHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperCacheGen: Fast Context Loading for Language Model ApplicationsYuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen JiangPaperContext Compression for Auto-regressive Transformers with Sentinel TokensSiyu Ren, Qi Jia, Kenny Q. ZhuGithubPaperA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language ModelsShengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido ZucconGithubPaperSPEED: Speculative Pipelined Execution for Efficient DecodingColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia ShaoPaperAccelerating LLM Inference by Enabling Intermediate Layer DecodingNeeraj Varshney, Agneet Chatterjee, Mihir Parmar, Chitta BaralPaperFast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers FasterHongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai ChenPaperCompressed Context Memory For Online Language Model InteractionJang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh SongGithubPaperSparQ Attention: Bandwidth-Efficient LLM InferenceLuka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas OrrPaperLookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation AccuracyYao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie GuPaperCascade Speculative Drafting for Even Faster LLM InferenceZiyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan ChangPaperEAGLE: Lossless Acceleration of LLM Decoding by Feature ExtrapolationYuhui Li, Chao Zhang, and Hongyang ZhangGithubBlogLoMA: Lossless Compressed Memory AttentionYumeng Wang, Zhenyang XiaoPaperMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri DaoGithubPaperAPAR: LLMs Can Do Auto-Parallel Auto-Regressive DecodingMingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao DongPaperBiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language ModelsFeng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong XiaoGithubPaperGet More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM InferenceHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi ChenGithubPaperSpeculative Streaming: Fast LLM Inference without Auxiliary ModelsNikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar NajibiPaperRelayAttention for Efficient Large Language Model Serving with Long System PromptsLei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. LauPaperRecursive Speculative Decoding: Accelerating LLM Inference via Sampling Without ReplacementWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher LottPaperChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase PartitionLu Ye, Ze Tao, Yong Huang, Yang LiPaperChimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all TokensZiqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen ChenGithubPaperGEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLMHao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo ZhaoGithubPaperCHAI: Clustered Head Attention for Efficient LLM InferenceSaurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean WuPaperDynamic Memory Compression: Retrofitting LLMs for Accelerated InferencePiotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, Edoardo M. PontiPaperKeyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative InferenceMuhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham KamathPaperRecurrent Drafter for Fast Speculative Decoding in Large Language ModelsAonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei ChengPaperOptimal Block-Level Draft Verification for Accelerating Speculative DecodingZiteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha SureshPaperHierarchical Skip Decoding for Efficient Autoregressive Text GenerationYunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng ZhangPaperALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV CachingYoupeng Zhao, Di Wu, Jun WangPaperSDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive TokensChengbo Liu, Yong ZhuGithubPaperPrepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language ModelsSiyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya GroverGithubPaperTowards Fast Inference: Exploring and Improving Blockwise Parallel DraftsTaehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, Adrian BentonPaperLossless Acceleration of Large Language Model via Adaptive N-gram Parallel DecodingJie Ou, Yueming Chen, Wenhong TianGithubPaperSelf-Selected Attention Span for Accelerating Large Language Model InferenceTian Jin, Wanzin Yazar, Zifei Xu, Sayeh Sharify, Xin WangPaperParallel Decoding via Hidden Transfer for Lossless Large Language Model AccelerationPengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan ZhaoPaperXC-Cache: Cross-Attending to Cached Context for Efficient LLM InferenceJo√£o Monteiro, √âtienne Marcotte, Pierre-Andr√© No√´l, Valentina Zantedeschi, David V√°zquez, Nicolas Chapados, Christopher Pal, Perouz TaslakianPaperHybrid LLM: Cost-Efficient and Quality-Aware Query RoutingDujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks V.S. Lakshmanan, Ahmed Hassan AwadallahGithubPaperEfficient LLM Inference with KcacheQiaozhi He, Zhihua WuPaperBetter & Faster Large Language Models via Multi-token PredictionFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, Gabriel SynnaevePaperKV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache GenerationMinsik Cho, Mohammad Rastegari, Devang NaikPaperYou Only Cache Once: Decoder-Decoder Architectures for Language ModelsYutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu WeiGithubPaperKangaroo: Lossless Self-Speculative Decoding via Double Early ExitingFangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe WangGithubPaperAccelerating Speculative Decoding using Dynamic Speculation LengthJonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy SchwartzPaperClover: Regressive Lightweight Speculative Decoding with Sequential KnowledgeBin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin CuiPaperEfficient MOETitle & Authors\\nIntroduction\\nLinksSiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts ModelsZhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai \"Helen\" Li, Yiran ChenPaperFast Inference of Mixture-of-Experts Language Models with OffloadingArtyom Eliseev, Denis MazurGithubPaperSwitchHead: Accelerating Transformers with Mixture-of-Experts AttentionR√≥bert Csord√°s, Piotr Piƒôkos, Kazuki Irie, J√ºrgen SchmidhuberGithubPaperExploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model InferenceJinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK)PandaGithubPaperMoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE ServingLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh MarinaGithubPaperFiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts ModelsKeisuke Kamahori, Yile Gu, Kan Zhu, Baris KasikciGithubPaperNot All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language ModelsXudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng LiGithubPaperEnhancing Efficiency in Sparse Models with Sparser SelectionYuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, Zenglin XuGithubPaperPrompt-prompted Mixture of Experts for Efficient LLM GenerationHarry Dong, Beidi Chen, Yuejie ChiGithubPaperShortcut-connected Expert Parallelism for Accelerating Mixture-of-ExpertsWeilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi HuangPaperSEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-ExpertsAlexandre Muzio, Alex Sun, Churan HePaperDense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language ModelsBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar PandaPaperLancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication OverlappingChenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, Yida WangPaperEfficient Architecture of LLMTitle & Authors\\nIntroduction\\nLinksRethinking Optimization and Architecture for Tiny Language ModelsYehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe WangGithubPaperTandem Transformers for Inference Efficient LLMsAishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperScaling Efficient LLMsB.N. KausikPaperMobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use CasesZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas ChandraPaperThink Big, Generate Quick: LLM-to-SLM for Fast Autoregressive DecodingBenjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami BejnordiPaperMobiLlama: Towards Accurate and Lightweight Fully Transparent GPTOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz KhanGithubPaper ModelGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar GulcehrePaperDiJiang: Efficient Large Language Models through Compact KernelizationHanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe WangGithubPaperMegalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthXuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting ZhouGithubPaperHierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMsWoomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo ShinGithubPaperText CompressionTitle & Authors\\nIntroduction\\nLinksEntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text CompressionAlexander Tsvetkov. Alon KipnisPaperLLMZip: Lossless Text Compression using Large Language ModelsChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas ShakkottaiPaper | Unofficial GithubAdapting Language Models to Compress ContextsAlexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi ChenGithubPaperIn-context Autoencoder for Context Compression in a Large Language ModelTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu WeiPaperNugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language ModelGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van DurmePaperBoosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context PruningXijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Mao YangPaperProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel DecodingShuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru HuangPaperLearning to Compress Prompt in Natural Language FormatsYu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia HuPaperLLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt CompressionZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin et alPaperPCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language ModelsJinyi Li, Yihuai Lan, Lei Wang, Hao WangGithubPaperPROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt CompressionMuhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di WangPaperTraining LLMs over Neurally Compressed TextBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah ConstantPaperAdapting LLMs for Efficient Context Processing through Soft Prompt CompressionCangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian FloydPaperRethinking LLM Memorization through the Lens of Adversarial CompressionAvi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico KolterGithubPaperProjectLow-Rank DecompositionTitle & Authors\\nIntroduction\\nLinksLoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse ApproximationYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo ZhaoGithubPaperMatrix Compression via Randomized Low Rank and Low Precision FactorizationRajarshi Saha, Varun Srivastava, Mert PilanciGithubPaperTensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train DecompositionMingxue Xu, Yao Lei Xu, Danilo P. MandicPaperLORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot CompressionAyush Kaushal, Tejas Vaidhya, Irina RishPaperProjectRethinking Compression: Reduced Order Modelling of Latent Features in Large Language ModelsArnav Chavan, Nahush Lele, Deepak GuptaGithubPaperData-free Weight Compress and Denoise for Large Language ModelsRunyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua LinPaperSVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model CompressionXin Wang, Yu Zheng, Zhongwei Wan, Mi ZhangGithubPaperHardware/SystemFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√©. [Paper][Github]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. Tri Dao. [Paper][Github]Efficiently Scaling Transformer Inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean. [Paper]FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher R√©, Ion Stoica, Ce Zhang. [Paper][Github]Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica. [Paper][Github]Efficient LLM Inference on CPUs. Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng. [Paper][Github]\\nEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei Xu. [Paper]GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models. Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan Lin. [Paper]\\nRethinking Memory and Communication Cost for Efficient Large Language Model Training. Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou. [Paper]\\nChameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models. Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso. [Paper]\\nFlashDecoding++: Faster Large Language Model Inference on GPUs. Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, Yu Wang. [Paper]Striped Attention: Faster Ring Attention for Causal Transformers. William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley. [Paper][Github]PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU. Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen. [Paper][Github]\\nLLM in a flash: Efficient Large Language Model Inference with Limited Memory. Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar. [Paper]\\nFlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA. Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, Yu Wang. [Paper]\\nEfficient LLM inference solution on Intel GPU. Hui Wu, Yi Gan, Feng Yuan, Jing Ma, Wei Zhu, Yutao Xu, Hong Zhu, Yuhua Zhu, Xiaoli Liu, Jinghui Gu. [Paper][Github]Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models. Shuming Shi, Enbo Zhao, Deng Cai, Leyang Cui, Xinting Huang, Huayang Li. [Paper][Github]DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, Yuxiong He. [Paper][Github]QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference. Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim. [Paper][Github]FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning. Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia. [Paper][Github]\\nBurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences. Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su. [Paper]Efficiently Programming Large Language Models using SGLang. Lianmin Zheng*, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, Ying Sheng*. [Paper] [Github]\\nMELTing point: Mobile Evaluation of Language Transformers. MELTing point: Mobile Evaluation of Language Transformers. [Paper]\\nDeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference. Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin. [Paper]\\nTransformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie. [Paper]\\nLoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism. Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin. [Paper]M√©lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity. Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica. [Paper][Github]\\nExpert Router: Orchestrating Efficient Language Model Inference through Prompt Classification. Josef Pichlmeier, Philipp Ross, Andre Luckow. [Paper]\\nEfficient and Economic Large Language Model Inference with Attention Offloading. Shaoyuan Chen, Yutong Lin, Mingxing Zhang, Yongwei Wu. [Paper]TuningCPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun. [Paper]ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models. Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo. [Paper][Github]\\nTRANSOM: An Efficient Fault-Tolerant System for Training LLMs. Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li. [Paper]\\nDEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. Devleena Das, Vivek Khetan. [Paper]LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. Jianxin Yang. [Paper][Github]Sparse Fine-tuning for Inference Acceleration of Large Language Models. Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh. [Paper][Github][Github]ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization. Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal. [Paper][Github]\\nTowards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper. Chengyu Wang, Junbing Yan, Wei Zhang, Jun Huang. [Paper]SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification. Yuntao Gui, Xiao Yan, Peiqi Yin, Han Yang, James Cheng. [Paper][Github]LoRA+: Efficient Low Rank Adaptation of Large Models. Soufiane Hayou, Nikhil Ghosh, Bin Yu. [Paper][Github]\\nSparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning. Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You. [Paper]DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation. Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee. [Paper][Github]\\nLoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models. Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang. [Paper]\\nParameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun)Zhang, Sai Qian Zhang. [Paper]AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis. Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. [Paper][Github]BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. Qijun Luo, Hengxu Yu, Xiao Li. [Paper][Github]\\nIntuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning. Yijiang Liu, Rongyu Zhang, Huanrui Yang, Kurt Keutzer, Yuan Du, Li Du, Shanghang Zhang. [Paper]Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning. Jing Xu, Jingzhao Zhang. [Paper][Github]SurveyA Survey on Model Compression for Large Language Models. Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang. [Paper]The Efficiency Spectrum of Large Language Models: An Algorithmic Survey. Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang. [Paper][Github]Efficient Large Language Models: A Survey. Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang. [Paper][Github]\\nTowards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia. [Paper]Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao. [Paper][Github]A Survey of Resource-efficient LLM and Multimodal Foundation Models. Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu. [Paper][Github]\\nA Survey on Hardware Accelerators for Large Language Models. Christoforos Kachris. [Paper]Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, Yunxin Liu. [Paper][Github]\\nA Comprehensive Survey of Compression Algorithms for Language Models. Seungcheol Park, Jaehyeon Choi, Sojin Lee, U Kang. [Paper]\\nA Survey on Transformer Compression. Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao. [Paper]\\nModel Compression and Efficient Inference for Large Language Models: A Survey. Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He. [Paper]A Survey on Knowledge Distillation of Large Language Models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou. [Paper][Github]Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui. [Paper][Github][Blog]Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. Arnav Chavan, Raghav Magazine, Shubham Kushwaha, M√©rouane Debbah, Deepak Gupta. [Paper][Github]\\nEfficient Prompting Methods for Large Language Models: A Survey. Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu. [Paper]\\nA Survey on Efficient Inference for Large Language Models. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang. [Paper]LeaderboardPlatform\\nAccessHuggingface LLM Perf Leaderboard\\n[Source]LLM Safety Leaderboard (for compressed models)}\\n[Source]LLMPerf Leaderboard\\n[Source]LLM API Hosts Leaderboard\\n[Source]ML.ENERGY Leaderboard\\n[Source]Models Leaderboard\\n[Source]Provider Leaderboard\\n[Source]AboutA curated list for Efficient Large Language ModelsTopicscompressionlanguage-modelknowledge-distillationmodel-quantizationpruning-algorithmsllmllm-compressionefficient-llmResourcesReadmeActivityStars824starsWatchers36watchingForks61forksReport repositoryReleasesNo releases publishedPackages0No packages publishedContributors8LanguagesPython\\n100.0%Footer¬© 2024 GitHub,\\xa0Inc.Footer navigationTermsPrivacySecurityStatusDocsContactManage cookiesDo not share my personal informationYou can‚Äôt perform that action at this time.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"..\\\\Utils\\\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "docs = text_splitter.create_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthSkip to main contentWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate> cs > arXiv:2404.08801Help | Advanced SearchAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull textSearchopen searchGOopen navigation menuquick linksLogin\\nHelp Pages\\nAboutComputer Science > Machine LearningarXiv:2404.08801 (cs)[Submitted on 12 Apr 2024 (v1), last revised 16 Apr 2024 (this version, v2)]\\nTitle:Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length\\nAuthors:Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authors\\nView PDF\\nHTML (experimental)Abstract:The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: this https URLComments:\\n9 pages, 6 figures and 8 tablesSubjects:Machine Learning (cs.LG); Computation and Language (cs.CL)Cite as:\\narXiv:2404.08801 [cs.LG](orarXiv:2404.08801v2 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2404.08801Focus to learn morearXiv-issued DOI via DataCiteSubmission history From: Xuezhe Ma [view email][v1]Fri, 12 Apr 2024 20:28:14 UTC (568 KB)\\n[v2]Tue, 16 Apr 2024 07:27:58 UTC (572 KB)Full-text links:\\nAccess Paper:View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authorsView PDFHTML (experimental)TeX SourceOther Formats\\nview licenseCurrent browse context: cs.LG<\\xa0prev|next\\xa0>new|recent|2404Change to browse by:cs\\ncs.CLReferences & CitationsNASA ADSGoogle Scholar\\nSemantic Scholara\\nexport BibTeX citation\\nLoading...BibTeX formatted citation\\n√óloading...Data provided by:BookmarkBibliographic ToolsBibliographic and Citation ToolsBibliographic Explorer ToggleBibliographic Explorer (What is the Explorer?)Litmaps ToggleLitmaps (What is Litmaps?)scite.ai Togglescite Smart Citations (What are Smart Citations?)Code, Data, MediaCode, Data and Media Associated with this ArticleLinks to Code ToggleCatalyzeX Code Finder for Papers (What is CatalyzeX?)DagsHub ToggleDagsHub (What is DagsHub?)GotitPub ToggleGotit.pub (What is GotitPub?)Links to Code TogglePapers with Code (What is Papers with Code?)ScienceCast ToggleScienceCast (What is ScienceCast?)DemosDemosReplicate ToggleReplicate (What is Replicate?)Spaces ToggleHugging Face Spaces (What is Spaces?)Spaces ToggleTXYZ.AI (What is TXYZ.AI?)Related PapersRecommenders and Search ToolsLink to Influence FlowerInfluence Flower (What are Influence Flowers?)Connected Papers ToggleConnected Papers (What is Connected Papers?)Core recommender toggleCORE Recommender (What is CORE?)IArxiv recommender toggleIArxiv Recommender\\n(What is IArxiv?)Author\\nVenue\\nInstitution\\nTopicAbout arXivLabsarXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.'),\n",
       " Document(page_content=\"arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.Which authors of this paper are endorsers? |Disable MathJax (What is MathJax?)About\\nHelpcontact arXivClick here to contact arXivContactsubscribe to arXiv mailingsClick here to subscribeSubscribeCopyright\\nPrivacy PolicyWeb Accessibility AssistancearXiv Operational StatusGet status notifications viaemailor slack\"),\n",
       " Document(page_content='GitHub - horseee/Awesome-Efficient-LLM: A curated list for Efficient Large Language ModelsSkip to contentNavigation MenuToggle navigationSign inProductActionsAutomate any workflowPackagesHost and manage packagesSecurityFind and fix vulnerabilitiesCodespacesInstant dev environmentsCopilotWrite better code with AICode reviewManage code changesIssuesPlan and track workDiscussionsCollaborate outside of codeExploreAll featuresDocumentationGitHub SkillsBlogSolutionsForEnterpriseTeamsStartupsEducationBy SolutionCI/CD & AutomationDevOpsDevSecOpsResourcesLearning PathwaysWhite papers, Ebooks, WebinarsCustomer StoriesPartnersOpen SourceGitHub SponsorsFund open source developersThe ReadME ProjectGitHub community articlesRepositoriesTopicsTrendingCollectionsPricingSearch or jump to...Search code, repositories, users, issues, pull requests...SearchClearSearch syntax tipsProvide feedbackWe read every piece of feedback, and take your input very seriously.Include my email address so I can be contactedCancelSubmit feedbackSaved searchesUse saved searches to filter your results more quicklyNameQueryTo see all available qualifiers, see our documentation.CancelCreate saved searchSign inSign upYou signed in with another tab or window. Reload to refresh your session.'),\n",
       " Document(page_content='You signed out in another tab or window. Reload to refresh your session.'),\n",
       " Document(page_content=\"You switched accounts on another tab or window. Reload to refresh your session.Dismiss alerthorseee/Awesome-Efficient-LLMPublicNotificationsFork61Star824A curated list for Efficient Large Language Models824stars61forksBranchesTagsActivityStarNotificationsCodeIssues\\n0Pull requests\\n1ActionsProjects\\n0SecurityInsightsAdditional navigation optionsCodeIssuesPull requestsActionsProjectsSecurityInsightshorseee/Awesome-Efficient-LLMThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History323 Commitsefficient_plmefficient_plmfiguresfiguresprojectprojectREADME.mdREADME.mdgenerate_item.pygenerate_item.pyView all filesRepository files navigationREADMEAwesome-Efficient-LLM\\nA curated list for Efficient Large Language Models:Knowledge Distillation\\nNetwork Pruning\\nQuantization\\nInference Acceleration\\nEfficient MOE\\nEfficient Architecture of LLM\\nText Compression\\nLow-Rank Decomposition\\nHardware/System\\nTuning\\nSurvey\\nLeaderboardüöÄ UpdatesSep 27, 2023: Add tagfor papers accepted at NeurIPS'23. Sep 6, 2023: Add a new subdirectory project/ to organize those projects that are designed for developing a lightweight LLM. July 11, 2023:\\nIn light of the numerous publications that conducts experiments using PLMs (such as BERT, BART) currently, a new subdirectory efficient_plm/ is created to house papers that are applicable to PLMs but have yet to be verified for their effectiveness on LLMs (not implying that they are not suitable on LLM).üíÆ Contributing\\nIf you'd like to include your paper, or need to update any details such as conference information or code URLs, please feel free to submit a pull request. You can generate the required markdown format for each paper by filling in the information in generate_item.py and execute python generate_item.py.\"),\n",
       " Document(page_content='We warmly appreciate your contributions to this list.'),\n",
       " Document(page_content='Alternatively, you can email me with the links to your paper and code, and I would add your paper to the list at my earliest convenience. Knowledge DistillationTitle & Authors\\nIntroduction\\nLinksSpecializing Smaller Language Models towards Multi-Step ReasoningYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar KhotGithubPaperDistilling Script Knowledge from Large Language Models for Constrained Language PlanningSiyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing YangGithubPaperSCOTT: Self-Consistent Chain-of-Thought DistillationPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang RenPaperDISCO: Distilling Counterfactuals with Large Language ModelsZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle RichardsonGithubPaperI2D2: Inductive Knowledge Distillation with NeuroLogic and Self-ImitationChandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin ChoiGithubPaperProjectSymbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-StepLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin ChoiGithubPaperCan Language Models Teach? Teacher Explanations Improve Student Performance via Theory of MindSwarnadeep Saha, Peter Hase, and Mohit BansalGithubPaperDialogue Chain-of-Thought Distillation for Commonsense-aware Conversational AgentsHyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung YeoPaperPromptMix: A Class Boundary Augmentation Method for Large Language Model DistillationGaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. LaradjiGithubPaperTurning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative DataYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan LiGithubPaperDemocratizing Reasoning Ability: Tailored Learning from Large Language ModelZhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi ZhangGithubPaperGKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language ModelShicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Yang Yang, Hongyin Tang, Keqing He, Jiahao Liu, Jingang Wang, Shu Zhao, Peng Zhang, Jie TangGithubPaperDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model SizesCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas PfisterGithubPaperRetrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model CompressionJiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui YanPaperCache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language ModelsIlias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion AndroutsopoulosGithubPaperEfficiently Distilling LLMs for Edge ApplicationsAchintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih LeePaperLaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri AjiGithub paperKnowledge Distillation of Large Language ModelsYuxian Gu, Li Dong, Furu Wei, Minlie HuangGithubPaperTeaching Small Language Models to ReasonLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn.PaperLarge Language Model Distillation Doesn\\'t Need a TeacherAnanya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz BeltagyGithub paperThe False Promise of Imitating Proprietary LLMsArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn SongPaperImpossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and ParaphrasingJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin ChoiGithub paperPaD: Program-aided Distillation Specializes Large Models in ReasoningXuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen ZhouPaperRLCD: Reinforcement Learning from Contrast Distillation for Language Model AlignmentKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong TianPaperSci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QAYuhan Ma, Haiqi Jiang, Chenyou FanPaperUniversalNER: Targeted Distillation from Large Language Models for Open Named Entity RecognitionWenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung PoonGithubPaperProjectBaby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penaltyInar Timiryasov, Jean-Loup TastetGithubPaperDistillSpec: Improving Speculative Decoding via Knowledge DistillationYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran√ßois Kagy, Rishabh AgarwalPaperZephyr: Direct Distillation of LM AlignmentLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas WolfGithubPaperTowards the Law of Capacity Gap in Distilling Language ModelsChen Zhang, Dawei Song, Zheyu Ye, Yan GaoGithubPaperUnlock the Power: Competitive Distillation for Multi-Modal Large Language ModelsXinwei Li, Li Lin, Shuai Wang, Chen QianPaperMixed Distillation Helps Smaller Language Model Better ReasoningLi Chenglin, Chen Qianglong, Wang Caiyu, Zhang YinPaperDistilling Event Sequence Knowledge From Large Language ModelsSomin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker, Jian NiPaperKnowledge Distillation for Closed-Source Language ModelsHongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan, Ji ZhangPaperImproving Small Language Models\\' Mathematical Reasoning via Equation-of-Thought DistillationXunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping WangPaperScavenging Hyena: Distilling Transformers into Long Convolution ModelsTokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence LiangPaperDistiLLM: Towards Streamlined Distillation for Large Language ModelsJongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young YunGithubPaperLarge Language Model Meets Graph Neural Network in Knowledge DistillationShengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin ChenPaperUnmemorization in Large Language Models via Self-Distillation and Deliberate ImaginationYijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan VuliƒáGithubPaperTowards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMsNicolas Boizard, Kevin El-Haddad, C√©line Hudelot, Pierre ColomboGithub GithubPaperModelRevisiting Knowledge Distillation for Autoregressive Language ModelsQihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng TaoPaperPromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt TuningGyeongman Kim, Doohyuk Jang, Eunho YangPaperSelf-Distillation Bridges Distribution Gap in Language Model Fine-TuningZhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei ChenPaperWisdom of Committee: Distilling from Foundation Model to Specialized Application ModelZichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe ZhaoPaperDivide-or-Conquer? Which Part Should You Distill Your LLM?Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe ZhangPaperDistillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and DistillationPhuc Phan, Hieu Tran, Long PhanGithubPaperLeveraging Zero-Shot Prompting for Efficient Language Model DistillationLukas V√∂ge, Vincent Gurgul, Stefan LessmannPaperMetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction TasksLetian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo ShangGithubPaperModelGecko: Versatile Text Embeddings Distilled from Large Language ModelsJinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer et alPaperRethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language ModelsTaiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai WongPaperBlog-Eng Blog-‰∏≠Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language ModelsXiaoshu Chen, Sihang Zhou, Ke Liang, Xinwang LiuPaperCompressing Long Context for Enhancing RAG with AMR-based Concept DistillationKaize Shi, Xueyao Sun, Qing Li, Guandong XuPaperNetwork PruningTitle & Authors\\nIntroduction\\nLinksSparseGPT: Massive Language Models Can Be Accurately Pruned in One-ShotElias Frantar, Dan AlistarhGithub paperLLM-Pruner: On the Structural Pruning of Large Language ModelsXinyin Ma, Gongfan Fang, Xinchao WangGithub paperThe Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that MatterAjay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang WangGithubPaperFlash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured SparsityHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon SongGithubPaperA Simple and Effective Pruning Approach for Large Language ModelsMingjie Sun, Zhuang Liu, Anna Bair, J. Zico KolterGithubPaperSheared LLaMA: Accelerating Language Model Pre-training via Structured PruningMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi ChenGithubPaperPlug-and-Play: An Efficient Post-training Pruning Method for Large Language ModelsYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio CannistraciGithubPaperFluctuation-based Adaptive Structured Pruning for Large Language ModelsYongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao WangGithubPaperNASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language ModelsJongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young YunGithubPaperLoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-TuningMingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan ZhuangPaperPruning Large Language Models via Accuracy PredictorYupeng Ji, Yibo Cao, Jiucai LiuPaperCompressing LLMs: The Truth is Rarely Pure and Never SimpleAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei YangPaperJunk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through SparsityLu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang WangGithubPaperOutlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High SparsityLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei LiuGithubPaperCompresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language ModelsSong Guo, Jiahang Xu, Li Lyna Zhang, Mao YangGithubPaperSparse Finetuning for Inference Acceleration of Large Language ModelsEldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan AlistarhGithubPaperReLU Strikes Back: Exploiting Activation Sparsity in Large Language ModelsIman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad FarajtabarPaperThe Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context LearningTian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina DziugaitePaperOne-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language ModelsHang Shao, Bei Liu, Yanmin QianPaperLoRAShear: Efficient Large Language Model Structured Pruning and Knowledge RecoveryTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming LiangGithubPaperDivergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantizationBj√∂rn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias A√üenmacher, Kristian KerstingGithubPaperBeyond Size: How Gradients Shape Pruning Decisions in Large Language ModelsRocktim Jyoti Das, Liqun Ma, Zhiqiang ShenGithubPaperDynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMsYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong JiGithubPaperE-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M SparsityYun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui KangPaperPERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMsMax Zimmer, Megi Andoni, Christoph Spiegel, Sebastian PokuttaGithubPaperFast and Optimal Weight Update for Pruned Large Language ModelsVladim√≠r Bo≈æaGithubPaperPruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-TuningAdib Hasan, Ileana Rugina, Alex WangGithubPaperSliceGPT: Compress Large Language Models by Deleting Rows and ColumnsSaleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James HensmanGithubPaperAPT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and InferenceBowen Zhao, Hannaneh Hajishirzi, Qingqing CaoPaperReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMsZhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong SunPaperEverybody Prune Now: Structured Pruning of LLMs with only Forward PassesLucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet TalwalkarGithubPaperAssessing the Brittleness of Safety Alignment via Pruning and Low-Rank ModificationsBoyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia et alGithubPaperProjectNutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language ModelsShengrui Li, Xueting Han, Jing BaiPaperLearn To be Efficient: Build Structured Sparsity in Large Language ModelsHaizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul PrakashPaperShortened LLaMA: A Simple Depth Pruning for Large Language ModelsBo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu SongGithubPaperSLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer BlocksJiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon KimGithubPaperHiRE: High Recall Approximate Top-k Estimation for Efficient LLM InferenceYashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperLaCo: Large Language Model Pruning via Layer CollapseYifei Yang, Zouying Cao, Hai ZhaoPaperProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language ModelsChenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li et alGithubPaper[Model-7B] [Model-13B]EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMsSong Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong JiGithubPaperBESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity AllocationPeng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping LuoGithubPaperShortGPT: Layers in Large Language Models are More Redundant Than You ExpectXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng ChenPaperEfficient Pruning of Large Language Model with Adaptive Estimation FusionJun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi WangPaperDecoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under CompressionJunyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie et alGithubPaperProjectCompressing Large Language Models by Streamlining the Unimportant LayerXiaodong Chen, Yuxuan Hu, Jing ZhangPaperMultilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language BehindHongchuan Zeng, Hongshen Xu, Lu Chen, Kai YuGithubPaperAccelerating Inference in Large Language Models with a Unified Layer Skipping StrategyYijin Liu, Fandong Meng, Jie ZhouGithubPaperLoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language ModelsGuangyan Li, Yongqiang Tang, Wensheng ZhangPaperCATS: Contextually-Aware Thresholding for Sparsity in Large Language ModelsJe-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia MirhoseiniPaperLayer Skip: Enabling Early Exit Inference and Self-Speculative DecodingMostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer et alPaperEnabling High-Sparsity Foundational Llama Models with Efficient Pretraining and DeploymentAbhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, Sean Lie, Mark KurtzPaperDependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language ModelsZhiyu Guo, Hidetaka Kamigaito, Taro WanatnabePaperMixture-of-Depths: Dynamically allocating compute in transformer-based language modelsDavid Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam SantoroPaperQuantizationTitle & Authors\\nIntroduction\\nLinksGPTQ: Accurate Post-Training Quantization for Generative Pre-trained TransformersElias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan AlistarhGithubPaperSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language ModelsGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song HanGithubPaperQLoRA: Efficient Finetuning of Quantized LLMsTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke ZettlemoyerGithub PaperQuIP: 2-Bit Quantization of Large Language Models With GuaranteesJerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De SaXQGithubPaperMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer QuantizationJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo LeePaperQuantizable Transformers: Removing Outliers by Helping Attention Heads Do NothingYelysei Bondarenko, Markus Nagel, Tijmen BlankevoortGithub PaperLLM-FP4: 4-Bit Floating-Point Quantized TransformersShih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting ChengGithubPaperEnhancing Computation Efficiency in Large Language Models through Weight and Activation QuantizationJangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook ChoiPaperAgile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the EdgeXuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, Yanzhi WangPaperOmniQuant: Omnidirectionally Calibrated Quantization for Large Language ModelsWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping LuoGithubPaperAffineQuant: Affine Transformation Quantization for Large Language ModelsYuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong JiGithubPaperGPT-Zip: Deep Compression of Finetuned Large Language ModelsBerivan Isik, Hermann Kumbong, Wanyi Ning, Xiaozhe Yao, Sanmi Koyejo, Ce ZhangPaperWatermarking LLMs with Weight QuantizationLinyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng QiuGithubPaperAWQ: Activation-aware Weight Quantization for LLM Compression and AccelerationJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song HanGithubPaperRPTQ: Reorder-based Post-training Quantization for Large Language ModelsZhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe WuGithub PaperZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank CompensationZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong HePaperSqueezeLLM: Dense-and-Sparse Quantization Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt KeutzerGithubPaperOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scalingXiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong LiuPaperInteger or Floating Point? New Outlooks for Low-Bit Quantization on Large Language ModelsYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi XuPaperLLM-QAT: Data-Free Quantization Aware Training for Large Language ModelsZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas ChandraPaperSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan AlistarhGithubPaperOWQ: Lessons learned from activation outliers for weight quantization in large language modelsChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok ParkGithubPaperDo Emergent Abilities Exist in Quantized Large Language Models: An Empirical StudyPeiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong WenGithubPaperZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point FormatsXiaoxia Wu, Zhewei Yao, Yuxiong HePaperFPTQ: Fine-grained Post-Training Quantization for Large Language ModelsQingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen XiePaperQuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive AlgorithmKayhan Behdin, Ayan Acharya, Aman Gupta, Qingquan Song, Siyu Zhu, Sathiya Keerthi, Rahul MazumderGithubPaperNorm Tweaking: High-performance Low-bit Quantization of Large Language ModelsLiang Li, Qingyuan Li, Bo Zhang, Xiangxiang ChuPaperOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMsWenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao LvGithubPaperQA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language ModelsYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, Qi TianGithubPaperModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular QuantizersJunjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr KuleshovPaperPB-LLM: Partially Binarized Large Language ModelsYuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen DongGithubPaperDual Grained Quantization: Efficient Fine-Grained Quantization for LLMLuoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou, Hong ZhouPaperQFT: Quantized Full-parameter Tuning of LLMs with Affordable ResourcesZhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt KeutzerPaperQLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language ModelsJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan ZhuangPaperLoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language ModelsYixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo ZhaoPaperTEQ: Trainable Equivalent Transformation for Quantization of LLMsWenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao ShenGithubPaperBitNet: Scaling 1-bit Transformers for Large Language ModelsHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu WeiPaperAtom: Low-bit Quantization for Efficient and Accurate LLM ServingYilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris KasikciPaperAWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language ModelsBaisong Li, Xingwang Wang, Haixiao XuPaperAFPQ: Asymmetric Floating Point Quantization for LLMsYijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi XuGithubPaperA Speed Odyssey for Deployable Quantization of LLMsQingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen XiePaperLQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model FinetuningHan Guo, Philip Greengard, Eric P. Xing, Yoon KimGithubPaperEnabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous DequantizationJinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao DaiPaperSmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLMJiayi Pan, Chengcan Wang, Kaifu Zheng, Yangguang Li, Zhenyu Wang, Bin FengGithubPaperZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative TasksXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei YaoGithubPaperExtreme Compression of Large Language Models via Additive QuantizationVage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan AlistarhGithubPaperFP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-DesignHaojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon SongPaperKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache QuantizationColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir GholamiGithubPaperL4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQHyesung Jeon, Yulhwa Kim, Jae-joon KimPaperQuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice CodebooksAlbert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De SaGithubPaperBiLLM: Pushing the Limit of Post-Training Quantization for LLMsWei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan QiGithubPaperAccurate LoRA-Finetuning Quantization of LLMs via Information RetentionHaotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele MagnoGithubPaperApiQ: Finetuning of 2-Bit Quantized Large Language ModelBaohao Liao, Christof MonzPaperTowards Next-Level Post-Training Quantization of Hyper-Scale TransformersJunhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon JeonPaperEdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the EdgeXuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi WangGithubPaperBitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-DistillationDayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi XuGithubPaperWKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains MoreYuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang NiePaperDB-LLM: Accurate Dual-Binarization for Efficient LLMsHong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng TaoPaperOneBit: Towards Extremely Low-bit Large Language ModelsYuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang ChePaperBitDelta: Your Fine-Tune May Only Be Worth One BitJames Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle CaiGithubPaperAny-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMsYeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. LeePaperAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language ModelsZiyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao YuPaperGPTVQ: The Blessing of Dimensionality for LLM QuantizationMart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul WhatmoughGithubPaperA Comprehensive Evaluation of Quantization Strategies for Large Language ModelsRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi XiongPaperThe Era of 1-bit LLMs: All Large Language Models are in 1.58 BitsShuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu WeiPaperEvaluating Quantized Large Language ModelsShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu WangGithubPaperNo Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision QuantizationJune Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo LeePaperFlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor QuantizationYi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin PanPaperQAQ: Quality Adaptive Quantization for LLM KV CacheShichen Dong, Wen Cheng, Jiayu Qin, Wei WangGithubPaperWhat Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of PerturbationZhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui YanPaperFrameQuant: Flexible Low-Bit Quantization for TransformersHarshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas SinghPaperQuaRot: Outlier-Free 4-Bit Inference in Rotated LLMsSaleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James HensmanGithubPaperAccurate Block Quantization in LLMs with OutliersNikita Trukhanov, Ilya SoloveychikPaperCherry on Top: Parameter Heterogeneity and Quantization in Large Language ModelsWanyun Cui, Qianle WangPaperIncreased LLM Vulnerabilities from Fine-tuning and QuantizationDivyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth HarshangiPaperQuantization of Large Language Models with an Overdetermined BasisDaniil Merkulov, Daria Cherniuk, Alexander Rudikov, Ivan Oseledets, Ekaterina Muravleva, Aleksandr Mikhalev, Boris KashinPaperdecoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating PointsYi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping Cai, Yang Zhang, Shouda LiuGithubPaperLossless and Near-Lossless Compression for Foundation ModelsMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, Danny HarnikPaperHow Good Are Low-bit Quantized LLaMA3 Models? An Empirical StudyWei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele MagnoGithubPaperModelWhen Quantization Affects Confidence of Large Language Models?Irina Proskurina, Luc Brun, Guillaume Metzler, Julien VelcinGithubPaperQServe: W4A8KV4 Quantization and System Co-design for Efficient LLM ServingYujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song HanGithubPaperInference AccelerationTitle & Authors\\nIntroduction\\nLinksDeja Vu: Contextual Sparsity for Efficient LLMs at Inference TimeZichang Liu, Jue WANG, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi ChenGithubPaperScissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test TimeZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali ShrivastavaPaperDynamic Context Pruning for Efficient and Interpretable Autoregressive TransformersSotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas HofmannPaperH2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language ModelsZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang, Beidi ChenGithubPaperLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperFast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel DecodingSangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young YunGithubPaperCompressing Context to Enhance Inference Efficiency of Large Language ModelsYucheng Li, Bo Dong, Chenghua Lin, Frank GuerinGithubPaperConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models InferenceZiqian Zeng, Yihuai Hong, Hongliang Dai, Huiping Zhuang, Cen ChenPaperAccelerating LLM Inference with Staged Speculative DecodingBenjamin Spector, Chris RePaperTCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost ReductionJunyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming QianPaperInference with Reference: Lossless Acceleration of Large Language ModelsNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, Furu WeiGithubpaperSpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree VerificationXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao JiaGithubpaperSkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM InferenceLuciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, Subhabrata MukherjeePaperSkeleton-of-Thought: Large Language Models Can Do Parallel DecodingXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu WangPaperDraft & Verify: Lossless Large Language Model Acceleration via Self-Speculative DecodingJun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad MehrotraGithubPaperEfficient Streaming Language Models with Attention SinksGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike LewisGithubPaper(Dynamic) Prompting might be all you need to repair Compressed LLMsDuc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang WangPaperModel Tells You What to Discard: Adaptive KV Cache Compression for LLMsSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng GaoPaperLarge Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient ReasoningMurong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu YaoGithubPaperLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt CompressionHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperCacheGen: Fast Context Loading for Language Model ApplicationsYuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen JiangPaperContext Compression for Auto-regressive Transformers with Sentinel TokensSiyu Ren, Qi Jia, Kenny Q. ZhuGithubPaperA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language ModelsShengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido ZucconGithubPaperSPEED: Speculative Pipelined Execution for Efficient DecodingColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia ShaoPaperAccelerating LLM Inference by Enabling Intermediate Layer DecodingNeeraj Varshney, Agneet Chatterjee, Mihir Parmar, Chitta BaralPaperFast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers FasterHongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai ChenPaperCompressed Context Memory For Online Language Model InteractionJang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh SongGithubPaperSparQ Attention: Bandwidth-Efficient LLM InferenceLuka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas OrrPaperLookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation AccuracyYao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie GuPaperCascade Speculative Drafting for Even Faster LLM InferenceZiyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan ChangPaperEAGLE: Lossless Acceleration of LLM Decoding by Feature ExtrapolationYuhui Li, Chao Zhang, and Hongyang ZhangGithubBlogLoMA: Lossless Compressed Memory AttentionYumeng Wang, Zhenyang XiaoPaperMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri DaoGithubPaperAPAR: LLMs Can Do Auto-Parallel Auto-Regressive DecodingMingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao DongPaperBiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language ModelsFeng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong XiaoGithubPaperGet More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM InferenceHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi ChenGithubPaperSpeculative Streaming: Fast LLM Inference without Auxiliary ModelsNikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar NajibiPaperRelayAttention for Efficient Large Language Model Serving with Long System PromptsLei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. LauPaperRecursive Speculative Decoding: Accelerating LLM Inference via Sampling Without ReplacementWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher LottPaperChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase PartitionLu Ye, Ze Tao, Yong Huang, Yang LiPaperChimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all TokensZiqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen ChenGithubPaperGEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLMHao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo ZhaoGithubPaperCHAI: Clustered Head Attention for Efficient LLM InferenceSaurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean WuPaperDynamic Memory Compression: Retrofitting LLMs for Accelerated InferencePiotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, Edoardo M. PontiPaperKeyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative InferenceMuhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham KamathPaperRecurrent Drafter for Fast Speculative Decoding in Large Language ModelsAonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei ChengPaperOptimal Block-Level Draft Verification for Accelerating Speculative DecodingZiteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha SureshPaperHierarchical Skip Decoding for Efficient Autoregressive Text GenerationYunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng ZhangPaperALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV CachingYoupeng Zhao, Di Wu, Jun WangPaperSDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive TokensChengbo Liu, Yong ZhuGithubPaperPrepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language ModelsSiyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya GroverGithubPaperTowards Fast Inference: Exploring and Improving Blockwise Parallel DraftsTaehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, Adrian BentonPaperLossless Acceleration of Large Language Model via Adaptive N-gram Parallel DecodingJie Ou, Yueming Chen, Wenhong TianGithubPaperSelf-Selected Attention Span for Accelerating Large Language Model InferenceTian Jin, Wanzin Yazar, Zifei Xu, Sayeh Sharify, Xin WangPaperParallel Decoding via Hidden Transfer for Lossless Large Language Model AccelerationPengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan ZhaoPaperXC-Cache: Cross-Attending to Cached Context for Efficient LLM InferenceJo√£o Monteiro, √âtienne Marcotte, Pierre-Andr√© No√´l, Valentina Zantedeschi, David V√°zquez, Nicolas Chapados, Christopher Pal, Perouz TaslakianPaperHybrid LLM: Cost-Efficient and Quality-Aware Query RoutingDujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks V.S. Lakshmanan, Ahmed Hassan AwadallahGithubPaperEfficient LLM Inference with KcacheQiaozhi He, Zhihua WuPaperBetter & Faster Large Language Models via Multi-token PredictionFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, Gabriel SynnaevePaperKV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache GenerationMinsik Cho, Mohammad Rastegari, Devang NaikPaperYou Only Cache Once: Decoder-Decoder Architectures for Language ModelsYutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu WeiGithubPaperKangaroo: Lossless Self-Speculative Decoding via Double Early ExitingFangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe WangGithubPaperAccelerating Speculative Decoding using Dynamic Speculation LengthJonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy SchwartzPaperClover: Regressive Lightweight Speculative Decoding with Sequential KnowledgeBin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin CuiPaperEfficient MOETitle & Authors\\nIntroduction\\nLinksSiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts ModelsZhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai \"Helen\" Li, Yiran ChenPaperFast Inference of Mixture-of-Experts Language Models with OffloadingArtyom Eliseev, Denis MazurGithubPaperSwitchHead: Accelerating Transformers with Mixture-of-Experts AttentionR√≥bert Csord√°s, Piotr Piƒôkos, Kazuki Irie, J√ºrgen SchmidhuberGithubPaperExploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model InferenceJinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK)PandaGithubPaperMoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE ServingLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh MarinaGithubPaperFiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts ModelsKeisuke Kamahori, Yile Gu, Kan Zhu, Baris KasikciGithubPaperNot All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language ModelsXudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng LiGithubPaperEnhancing Efficiency in Sparse Models with Sparser SelectionYuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, Zenglin XuGithubPaperPrompt-prompted Mixture of Experts for Efficient LLM GenerationHarry Dong, Beidi Chen, Yuejie ChiGithubPaperShortcut-connected Expert Parallelism for Accelerating Mixture-of-ExpertsWeilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi HuangPaperSEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-ExpertsAlexandre Muzio, Alex Sun, Churan HePaperDense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language ModelsBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar PandaPaperLancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication OverlappingChenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, Yida WangPaperEfficient Architecture of LLMTitle & Authors\\nIntroduction\\nLinksRethinking Optimization and Architecture for Tiny Language ModelsYehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe WangGithubPaperTandem Transformers for Inference Efficient LLMsAishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperScaling Efficient LLMsB.N. KausikPaperMobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use CasesZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas ChandraPaperThink Big, Generate Quick: LLM-to-SLM for Fast Autoregressive DecodingBenjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami BejnordiPaperMobiLlama: Towards Accurate and Lightweight Fully Transparent GPTOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz KhanGithubPaper ModelGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar GulcehrePaperDiJiang: Efficient Large Language Models through Compact KernelizationHanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe WangGithubPaperMegalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthXuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting ZhouGithubPaperHierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMsWoomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo ShinGithubPaperText CompressionTitle & Authors\\nIntroduction\\nLinksEntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text CompressionAlexander Tsvetkov. Alon KipnisPaperLLMZip: Lossless Text Compression using Large Language ModelsChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas ShakkottaiPaper | Unofficial GithubAdapting Language Models to Compress ContextsAlexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi ChenGithubPaperIn-context Autoencoder for Context Compression in a Large Language ModelTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu WeiPaperNugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language ModelGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van DurmePaperBoosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context PruningXijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Mao YangPaperProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel DecodingShuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru HuangPaperLearning to Compress Prompt in Natural Language FormatsYu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia HuPaperLLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt CompressionZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin et alPaperPCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language ModelsJinyi Li, Yihuai Lan, Lei Wang, Hao WangGithubPaperPROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt CompressionMuhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di WangPaperTraining LLMs over Neurally Compressed TextBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah ConstantPaperAdapting LLMs for Efficient Context Processing through Soft Prompt CompressionCangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian FloydPaperRethinking LLM Memorization through the Lens of Adversarial CompressionAvi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico KolterGithubPaperProjectLow-Rank DecompositionTitle & Authors\\nIntroduction\\nLinksLoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse ApproximationYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo ZhaoGithubPaperMatrix Compression via Randomized Low Rank and Low Precision FactorizationRajarshi Saha, Varun Srivastava, Mert PilanciGithubPaperTensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train DecompositionMingxue Xu, Yao Lei Xu, Danilo P. MandicPaperLORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot CompressionAyush Kaushal, Tejas Vaidhya, Irina RishPaperProjectRethinking Compression: Reduced Order Modelling of Latent Features in Large Language ModelsArnav Chavan, Nahush Lele, Deepak GuptaGithubPaperData-free Weight Compress and Denoise for Large Language ModelsRunyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua LinPaperSVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model CompressionXin Wang, Yu Zheng, Zhongwei Wan, Mi ZhangGithubPaperHardware/SystemFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Tri Dao, Daniel Y.'),\n",
       " Document(page_content='Fu, Stefano Ermon, Atri Rudra, Christopher R√©. [Paper][Github]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. Tri Dao. [Paper][Github]Efficiently Scaling Transformer Inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean. [Paper]FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y.'),\n",
       " Document(page_content='Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E.'),\n",
       " Document(page_content='Gonzalez, Percy Liang, Christopher R√©, Ion Stoica, Ce Zhang. [Paper][Github]Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica. [Paper][Github]Efficient LLM Inference on CPUs. Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng. [Paper][Github]\\nEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei Xu. [Paper]GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models. Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan Lin. [Paper]\\nRethinking Memory and Communication Cost for Efficient Large Language Model Training. Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou. [Paper]\\nChameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models. Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso. [Paper]\\nFlashDecoding++: Faster Large Language Model Inference on GPUs. Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, Yu Wang. [Paper]Striped Attention: Faster Ring Attention for Causal Transformers. William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley. [Paper][Github]PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU. Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen. [Paper][Github]\\nLLM in a flash: Efficient Large Language Model Inference with Limited Memory. Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar. [Paper]\\nFlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA. Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, Yu Wang. [Paper]\\nEfficient LLM inference solution on Intel GPU. Hui Wu, Yi Gan, Feng Yuan, Jing Ma, Wei Zhu, Yutao Xu, Hong Zhu, Yuhua Zhu, Xiaoli Liu, Jinghui Gu. [Paper][Github]Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models. Shuming Shi, Enbo Zhao, Deng Cai, Leyang Cui, Xinting Huang, Huayang Li. [Paper][Github]DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, Yuxiong He. [Paper][Github]QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference. Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim. [Paper][Github]FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning. Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia. [Paper][Github]\\nBurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences. Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su. [Paper]Efficiently Programming Large Language Models using SGLang. Lianmin Zheng*, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E.'),\n",
       " Document(page_content='Gonzalez, Clark Barrett, Ying Sheng*. [Paper] [Github]\\nMELTing point: Mobile Evaluation of Language Transformers. MELTing point: Mobile Evaluation of Language Transformers. [Paper]\\nDeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference. Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin. [Paper]\\nTransformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie. [Paper]\\nLoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism. Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin. [Paper]M√©lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity. Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica. [Paper][Github]\\nExpert Router: Orchestrating Efficient Language Model Inference through Prompt Classification. Josef Pichlmeier, Philipp Ross, Andre Luckow. [Paper]\\nEfficient and Economic Large Language Model Inference with Attention Offloading. Shaoyuan Chen, Yutong Lin, Mingxing Zhang, Yongwei Wu. [Paper]TuningCPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun. [Paper]ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models. Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo. [Paper][Github]\\nTRANSOM: An Efficient Fault-Tolerant System for Training LLMs. Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li. [Paper]\\nDEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. Devleena Das, Vivek Khetan. [Paper]LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. Jianxin Yang. [Paper][Github]Sparse Fine-tuning for Inference Acceleration of Large Language Models. Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh. [Paper][Github][Github]ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization. Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal. [Paper][Github]\\nTowards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper. Chengyu Wang, Junbing Yan, Wei Zhang, Jun Huang. [Paper]SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification. Yuntao Gui, Xiao Yan, Peiqi Yin, Han Yang, James Cheng. [Paper][Github]LoRA+: Efficient Low Rank Adaptation of Large Models. Soufiane Hayou, Nikhil Ghosh, Bin Yu. [Paper][Github]\\nSparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning. Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You. [Paper]DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation. Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee. [Paper][Github]\\nLoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models. Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang. [Paper]\\nParameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun)Zhang, Sai Qian Zhang. [Paper]AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis. Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. [Paper][Github]BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. Qijun Luo, Hengxu Yu, Xiao Li. [Paper][Github]\\nIntuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning. Yijiang Liu, Rongyu Zhang, Huanrui Yang, Kurt Keutzer, Yuan Du, Li Du, Shanghang Zhang. [Paper]Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning. Jing Xu, Jingzhao Zhang. [Paper][Github]SurveyA Survey on Model Compression for Large Language Models. Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang. [Paper]The Efficiency Spectrum of Large Language Models: An Algorithmic Survey. Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang. [Paper][Github]Efficient Large Language Models: A Survey. Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang. [Paper][Github]\\nTowards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia. [Paper]Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao. [Paper][Github]A Survey of Resource-efficient LLM and Multimodal Foundation Models. Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu. [Paper][Github]\\nA Survey on Hardware Accelerators for Large Language Models. Christoforos Kachris.'),\n",
       " Document(page_content='[Paper]Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, Yunxin Liu. [Paper][Github]\\nA Comprehensive Survey of Compression Algorithms for Language Models. Seungcheol Park, Jaehyeon Choi, Sojin Lee, U Kang. [Paper]\\nA Survey on Transformer Compression. Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao. [Paper]\\nModel Compression and Efficient Inference for Large Language Models: A Survey. Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He. [Paper]A Survey on Knowledge Distillation of Large Language Models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou. [Paper][Github]Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui. [Paper][Github][Blog]Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. Arnav Chavan, Raghav Magazine, Shubham Kushwaha, M√©rouane Debbah, Deepak Gupta. [Paper][Github]\\nEfficient Prompting Methods for Large Language Models: A Survey. Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu. [Paper]\\nA Survey on Efficient Inference for Large Language Models. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang. [Paper]LeaderboardPlatform\\nAccessHuggingface LLM Perf Leaderboard\\n[Source]LLM Safety Leaderboard (for compressed models)}\\n[Source]LLMPerf Leaderboard\\n[Source]LLM API Hosts Leaderboard\\n[Source]ML.ENERGY Leaderboard\\n[Source]Models Leaderboard\\n[Source]Provider Leaderboard\\n[Source]AboutA curated list for Efficient Large Language ModelsTopicscompressionlanguage-modelknowledge-distillationmodel-quantizationpruning-algorithmsllmllm-compressionefficient-llmResourcesReadmeActivityStars824starsWatchers36watchingForks61forksReport repositoryReleasesNo releases publishedPackages0No packages publishedContributors8LanguagesPython\\n100.0%Footer¬© 2024 GitHub,\\xa0Inc.Footer navigationTermsPrivacySecurityStatusDocsContactManage cookiesDo not share my personal informationYou can‚Äôt perform that action at this time.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"tell me about Megalodon in detail\")\n",
    "texts = [doc.page_content for doc in docs[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthSkip to main contentWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate> cs > arXiv:2404.08801Help | Advanced SearchAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull textSearchopen searchGOopen navigation menuquick linksLogin\\nHelp Pages\\nAboutComputer Science > Machine LearningarXiv:2404.08801 (cs)[Submitted on 12 Apr 2024 (v1), last revised 16 Apr 2024 (this version, v2)]\\nTitle:Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length\\nAuthors:Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authors\\nView PDF\\nHTML (experimental)Abstract:The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: this https URLComments:\\n9 pages, 6 figures and 8 tablesSubjects:Machine Learning (cs.LG); Computation and Language (cs.CL)Cite as:\\narXiv:2404.08801 [cs.LG](orarXiv:2404.08801v2 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2404.08801Focus to learn morearXiv-issued DOI via DataCiteSubmission history From: Xuezhe Ma [view email][v1]Fri, 12 Apr 2024 20:28:14 UTC (568 KB)\\n[v2]Tue, 16 Apr 2024 07:27:58 UTC (572 KB)Full-text links:\\nAccess Paper:View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authorsView PDFHTML (experimental)TeX SourceOther Formats\\nview licenseCurrent browse context: cs.LG<\\xa0prev|next\\xa0>new|recent|2404Change to browse by:cs\\ncs.CLReferences & CitationsNASA ADSGoogle Scholar\\nSemantic Scholara\\nexport BibTeX citation\\nLoading...BibTeX formatted citation\\n√óloading...Data provided by:BookmarkBibliographic ToolsBibliographic and Citation ToolsBibliographic Explorer ToggleBibliographic Explorer (What is the Explorer?)Litmaps ToggleLitmaps (What is Litmaps?)scite.ai Togglescite Smart Citations (What are Smart Citations?)Code, Data, MediaCode, Data and Media Associated with this ArticleLinks to Code ToggleCatalyzeX Code Finder for Papers (What is CatalyzeX?)DagsHub ToggleDagsHub (What is DagsHub?)GotitPub ToggleGotit.pub (What is GotitPub?)Links to Code TogglePapers with Code (What is Papers with Code?)ScienceCast ToggleScienceCast (What is ScienceCast?)DemosDemosReplicate ToggleReplicate (What is Replicate?)Spaces ToggleHugging Face Spaces (What is Spaces?)Spaces ToggleTXYZ.AI (What is TXYZ.AI?)Related PapersRecommenders and Search ToolsLink to Influence FlowerInfluence Flower (What are Influence Flowers?)Connected Papers ToggleConnected Papers (What is Connected Papers?)Core recommender toggleCORE Recommender (What is CORE?)IArxiv recommender toggleIArxiv Recommender\\n(What is IArxiv?)Author\\nVenue\\nInstitution\\nTopicAbout arXivLabsarXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy.',\n",
       " 'Alternatively, you can email me with the links to your paper and code, and I would add your paper to the list at my earliest convenience. Knowledge DistillationTitle & Authors\\nIntroduction\\nLinksSpecializing Smaller Language Models towards Multi-Step ReasoningYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar KhotGithubPaperDistilling Script Knowledge from Large Language Models for Constrained Language PlanningSiyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing YangGithubPaperSCOTT: Self-Consistent Chain-of-Thought DistillationPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang RenPaperDISCO: Distilling Counterfactuals with Large Language ModelsZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle RichardsonGithubPaperI2D2: Inductive Knowledge Distillation with NeuroLogic and Self-ImitationChandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin ChoiGithubPaperProjectSymbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-StepLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin ChoiGithubPaperCan Language Models Teach? Teacher Explanations Improve Student Performance via Theory of MindSwarnadeep Saha, Peter Hase, and Mohit BansalGithubPaperDialogue Chain-of-Thought Distillation for Commonsense-aware Conversational AgentsHyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung YeoPaperPromptMix: A Class Boundary Augmentation Method for Large Language Model DistillationGaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. LaradjiGithubPaperTurning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative DataYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan LiGithubPaperDemocratizing Reasoning Ability: Tailored Learning from Large Language ModelZhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi ZhangGithubPaperGKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language ModelShicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Yang Yang, Hongyin Tang, Keqing He, Jiahao Liu, Jingang Wang, Shu Zhao, Peng Zhang, Jie TangGithubPaperDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model SizesCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas PfisterGithubPaperRetrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model CompressionJiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui YanPaperCache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language ModelsIlias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion AndroutsopoulosGithubPaperEfficiently Distilling LLMs for Edge ApplicationsAchintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih LeePaperLaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri AjiGithub paperKnowledge Distillation of Large Language ModelsYuxian Gu, Li Dong, Furu Wei, Minlie HuangGithubPaperTeaching Small Language Models to ReasonLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn.PaperLarge Language Model Distillation Doesn\\'t Need a TeacherAnanya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz BeltagyGithub paperThe False Promise of Imitating Proprietary LLMsArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn SongPaperImpossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and ParaphrasingJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin ChoiGithub paperPaD: Program-aided Distillation Specializes Large Models in ReasoningXuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen ZhouPaperRLCD: Reinforcement Learning from Contrast Distillation for Language Model AlignmentKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong TianPaperSci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QAYuhan Ma, Haiqi Jiang, Chenyou FanPaperUniversalNER: Targeted Distillation from Large Language Models for Open Named Entity RecognitionWenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung PoonGithubPaperProjectBaby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penaltyInar Timiryasov, Jean-Loup TastetGithubPaperDistillSpec: Improving Speculative Decoding via Knowledge DistillationYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran√ßois Kagy, Rishabh AgarwalPaperZephyr: Direct Distillation of LM AlignmentLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas WolfGithubPaperTowards the Law of Capacity Gap in Distilling Language ModelsChen Zhang, Dawei Song, Zheyu Ye, Yan GaoGithubPaperUnlock the Power: Competitive Distillation for Multi-Modal Large Language ModelsXinwei Li, Li Lin, Shuai Wang, Chen QianPaperMixed Distillation Helps Smaller Language Model Better ReasoningLi Chenglin, Chen Qianglong, Wang Caiyu, Zhang YinPaperDistilling Event Sequence Knowledge From Large Language ModelsSomin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker, Jian NiPaperKnowledge Distillation for Closed-Source Language ModelsHongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan, Ji ZhangPaperImproving Small Language Models\\' Mathematical Reasoning via Equation-of-Thought DistillationXunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping WangPaperScavenging Hyena: Distilling Transformers into Long Convolution ModelsTokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence LiangPaperDistiLLM: Towards Streamlined Distillation for Large Language ModelsJongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young YunGithubPaperLarge Language Model Meets Graph Neural Network in Knowledge DistillationShengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin ChenPaperUnmemorization in Large Language Models via Self-Distillation and Deliberate ImaginationYijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan VuliƒáGithubPaperTowards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMsNicolas Boizard, Kevin El-Haddad, C√©line Hudelot, Pierre ColomboGithub GithubPaperModelRevisiting Knowledge Distillation for Autoregressive Language ModelsQihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng TaoPaperPromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt TuningGyeongman Kim, Doohyuk Jang, Eunho YangPaperSelf-Distillation Bridges Distribution Gap in Language Model Fine-TuningZhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei ChenPaperWisdom of Committee: Distilling from Foundation Model to Specialized Application ModelZichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe ZhaoPaperDivide-or-Conquer? Which Part Should You Distill Your LLM?Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe ZhangPaperDistillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and DistillationPhuc Phan, Hieu Tran, Long PhanGithubPaperLeveraging Zero-Shot Prompting for Efficient Language Model DistillationLukas V√∂ge, Vincent Gurgul, Stefan LessmannPaperMetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction TasksLetian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo ShangGithubPaperModelGecko: Versatile Text Embeddings Distilled from Large Language ModelsJinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer et alPaperRethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language ModelsTaiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai WongPaperBlog-Eng Blog-‰∏≠Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language ModelsXiaoshu Chen, Sihang Zhou, Ke Liang, Xinwang LiuPaperCompressing Long Context for Enhancing RAG with AMR-based Concept DistillationKaize Shi, Xueyao Sun, Qing Li, Guandong XuPaperNetwork PruningTitle & Authors\\nIntroduction\\nLinksSparseGPT: Massive Language Models Can Be Accurately Pruned in One-ShotElias Frantar, Dan AlistarhGithub paperLLM-Pruner: On the Structural Pruning of Large Language ModelsXinyin Ma, Gongfan Fang, Xinchao WangGithub paperThe Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that MatterAjay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang WangGithubPaperFlash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured SparsityHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon SongGithubPaperA Simple and Effective Pruning Approach for Large Language ModelsMingjie Sun, Zhuang Liu, Anna Bair, J. Zico KolterGithubPaperSheared LLaMA: Accelerating Language Model Pre-training via Structured PruningMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi ChenGithubPaperPlug-and-Play: An Efficient Post-training Pruning Method for Large Language ModelsYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio CannistraciGithubPaperFluctuation-based Adaptive Structured Pruning for Large Language ModelsYongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao WangGithubPaperNASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language ModelsJongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young YunGithubPaperLoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-TuningMingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan ZhuangPaperPruning Large Language Models via Accuracy PredictorYupeng Ji, Yibo Cao, Jiucai LiuPaperCompressing LLMs: The Truth is Rarely Pure and Never SimpleAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei YangPaperJunk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through SparsityLu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang WangGithubPaperOutlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High SparsityLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei LiuGithubPaperCompresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language ModelsSong Guo, Jiahang Xu, Li Lyna Zhang, Mao YangGithubPaperSparse Finetuning for Inference Acceleration of Large Language ModelsEldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan AlistarhGithubPaperReLU Strikes Back: Exploiting Activation Sparsity in Large Language ModelsIman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad FarajtabarPaperThe Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context LearningTian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina DziugaitePaperOne-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language ModelsHang Shao, Bei Liu, Yanmin QianPaperLoRAShear: Efficient Large Language Model Structured Pruning and Knowledge RecoveryTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming LiangGithubPaperDivergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantizationBj√∂rn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias A√üenmacher, Kristian KerstingGithubPaperBeyond Size: How Gradients Shape Pruning Decisions in Large Language ModelsRocktim Jyoti Das, Liqun Ma, Zhiqiang ShenGithubPaperDynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMsYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong JiGithubPaperE-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M SparsityYun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui KangPaperPERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMsMax Zimmer, Megi Andoni, Christoph Spiegel, Sebastian PokuttaGithubPaperFast and Optimal Weight Update for Pruned Large Language ModelsVladim√≠r Bo≈æaGithubPaperPruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-TuningAdib Hasan, Ileana Rugina, Alex WangGithubPaperSliceGPT: Compress Large Language Models by Deleting Rows and ColumnsSaleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James HensmanGithubPaperAPT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and InferenceBowen Zhao, Hannaneh Hajishirzi, Qingqing CaoPaperReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMsZhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong SunPaperEverybody Prune Now: Structured Pruning of LLMs with only Forward PassesLucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet TalwalkarGithubPaperAssessing the Brittleness of Safety Alignment via Pruning and Low-Rank ModificationsBoyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia et alGithubPaperProjectNutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language ModelsShengrui Li, Xueting Han, Jing BaiPaperLearn To be Efficient: Build Structured Sparsity in Large Language ModelsHaizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul PrakashPaperShortened LLaMA: A Simple Depth Pruning for Large Language ModelsBo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu SongGithubPaperSLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer BlocksJiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon KimGithubPaperHiRE: High Recall Approximate Top-k Estimation for Efficient LLM InferenceYashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperLaCo: Large Language Model Pruning via Layer CollapseYifei Yang, Zouying Cao, Hai ZhaoPaperProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language ModelsChenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li et alGithubPaper[Model-7B] [Model-13B]EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMsSong Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong JiGithubPaperBESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity AllocationPeng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping LuoGithubPaperShortGPT: Layers in Large Language Models are More Redundant Than You ExpectXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng ChenPaperEfficient Pruning of Large Language Model with Adaptive Estimation FusionJun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi WangPaperDecoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under CompressionJunyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie et alGithubPaperProjectCompressing Large Language Models by Streamlining the Unimportant LayerXiaodong Chen, Yuxuan Hu, Jing ZhangPaperMultilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language BehindHongchuan Zeng, Hongshen Xu, Lu Chen, Kai YuGithubPaperAccelerating Inference in Large Language Models with a Unified Layer Skipping StrategyYijin Liu, Fandong Meng, Jie ZhouGithubPaperLoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language ModelsGuangyan Li, Yongqiang Tang, Wensheng ZhangPaperCATS: Contextually-Aware Thresholding for Sparsity in Large Language ModelsJe-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia MirhoseiniPaperLayer Skip: Enabling Early Exit Inference and Self-Speculative DecodingMostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer et alPaperEnabling High-Sparsity Foundational Llama Models with Efficient Pretraining and DeploymentAbhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, Sean Lie, Mark KurtzPaperDependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language ModelsZhiyu Guo, Hidetaka Kamigaito, Taro WanatnabePaperMixture-of-Depths: Dynamically allocating compute in transformer-based language modelsDavid Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam SantoroPaperQuantizationTitle & Authors\\nIntroduction\\nLinksGPTQ: Accurate Post-Training Quantization for Generative Pre-trained TransformersElias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan AlistarhGithubPaperSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language ModelsGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song HanGithubPaperQLoRA: Efficient Finetuning of Quantized LLMsTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke ZettlemoyerGithub PaperQuIP: 2-Bit Quantization of Large Language Models With GuaranteesJerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De SaXQGithubPaperMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer QuantizationJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo LeePaperQuantizable Transformers: Removing Outliers by Helping Attention Heads Do NothingYelysei Bondarenko, Markus Nagel, Tijmen BlankevoortGithub PaperLLM-FP4: 4-Bit Floating-Point Quantized TransformersShih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting ChengGithubPaperEnhancing Computation Efficiency in Large Language Models through Weight and Activation QuantizationJangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook ChoiPaperAgile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the EdgeXuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, Yanzhi WangPaperOmniQuant: Omnidirectionally Calibrated Quantization for Large Language ModelsWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping LuoGithubPaperAffineQuant: Affine Transformation Quantization for Large Language ModelsYuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong JiGithubPaperGPT-Zip: Deep Compression of Finetuned Large Language ModelsBerivan Isik, Hermann Kumbong, Wanyi Ning, Xiaozhe Yao, Sanmi Koyejo, Ce ZhangPaperWatermarking LLMs with Weight QuantizationLinyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng QiuGithubPaperAWQ: Activation-aware Weight Quantization for LLM Compression and AccelerationJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song HanGithubPaperRPTQ: Reorder-based Post-training Quantization for Large Language ModelsZhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe WuGithub PaperZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank CompensationZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong HePaperSqueezeLLM: Dense-and-Sparse Quantization Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt KeutzerGithubPaperOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scalingXiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong LiuPaperInteger or Floating Point? New Outlooks for Low-Bit Quantization on Large Language ModelsYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi XuPaperLLM-QAT: Data-Free Quantization Aware Training for Large Language ModelsZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas ChandraPaperSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan AlistarhGithubPaperOWQ: Lessons learned from activation outliers for weight quantization in large language modelsChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok ParkGithubPaperDo Emergent Abilities Exist in Quantized Large Language Models: An Empirical StudyPeiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong WenGithubPaperZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point FormatsXiaoxia Wu, Zhewei Yao, Yuxiong HePaperFPTQ: Fine-grained Post-Training Quantization for Large Language ModelsQingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen XiePaperQuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive AlgorithmKayhan Behdin, Ayan Acharya, Aman Gupta, Qingquan Song, Siyu Zhu, Sathiya Keerthi, Rahul MazumderGithubPaperNorm Tweaking: High-performance Low-bit Quantization of Large Language ModelsLiang Li, Qingyuan Li, Bo Zhang, Xiangxiang ChuPaperOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMsWenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao LvGithubPaperQA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language ModelsYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, Qi TianGithubPaperModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular QuantizersJunjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr KuleshovPaperPB-LLM: Partially Binarized Large Language ModelsYuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen DongGithubPaperDual Grained Quantization: Efficient Fine-Grained Quantization for LLMLuoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou, Hong ZhouPaperQFT: Quantized Full-parameter Tuning of LLMs with Affordable ResourcesZhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt KeutzerPaperQLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language ModelsJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan ZhuangPaperLoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language ModelsYixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo ZhaoPaperTEQ: Trainable Equivalent Transformation for Quantization of LLMsWenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao ShenGithubPaperBitNet: Scaling 1-bit Transformers for Large Language ModelsHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu WeiPaperAtom: Low-bit Quantization for Efficient and Accurate LLM ServingYilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris KasikciPaperAWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language ModelsBaisong Li, Xingwang Wang, Haixiao XuPaperAFPQ: Asymmetric Floating Point Quantization for LLMsYijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi XuGithubPaperA Speed Odyssey for Deployable Quantization of LLMsQingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen XiePaperLQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model FinetuningHan Guo, Philip Greengard, Eric P. Xing, Yoon KimGithubPaperEnabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous DequantizationJinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao DaiPaperSmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLMJiayi Pan, Chengcan Wang, Kaifu Zheng, Yangguang Li, Zhenyu Wang, Bin FengGithubPaperZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative TasksXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei YaoGithubPaperExtreme Compression of Large Language Models via Additive QuantizationVage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan AlistarhGithubPaperFP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-DesignHaojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon SongPaperKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache QuantizationColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir GholamiGithubPaperL4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQHyesung Jeon, Yulhwa Kim, Jae-joon KimPaperQuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice CodebooksAlbert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De SaGithubPaperBiLLM: Pushing the Limit of Post-Training Quantization for LLMsWei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan QiGithubPaperAccurate LoRA-Finetuning Quantization of LLMs via Information RetentionHaotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele MagnoGithubPaperApiQ: Finetuning of 2-Bit Quantized Large Language ModelBaohao Liao, Christof MonzPaperTowards Next-Level Post-Training Quantization of Hyper-Scale TransformersJunhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon JeonPaperEdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the EdgeXuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi WangGithubPaperBitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-DistillationDayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi XuGithubPaperWKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains MoreYuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang NiePaperDB-LLM: Accurate Dual-Binarization for Efficient LLMsHong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng TaoPaperOneBit: Towards Extremely Low-bit Large Language ModelsYuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang ChePaperBitDelta: Your Fine-Tune May Only Be Worth One BitJames Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle CaiGithubPaperAny-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMsYeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. LeePaperAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language ModelsZiyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao YuPaperGPTVQ: The Blessing of Dimensionality for LLM QuantizationMart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul WhatmoughGithubPaperA Comprehensive Evaluation of Quantization Strategies for Large Language ModelsRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi XiongPaperThe Era of 1-bit LLMs: All Large Language Models are in 1.58 BitsShuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu WeiPaperEvaluating Quantized Large Language ModelsShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu WangGithubPaperNo Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision QuantizationJune Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo LeePaperFlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor QuantizationYi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin PanPaperQAQ: Quality Adaptive Quantization for LLM KV CacheShichen Dong, Wen Cheng, Jiayu Qin, Wei WangGithubPaperWhat Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of PerturbationZhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui YanPaperFrameQuant: Flexible Low-Bit Quantization for TransformersHarshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas SinghPaperQuaRot: Outlier-Free 4-Bit Inference in Rotated LLMsSaleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James HensmanGithubPaperAccurate Block Quantization in LLMs with OutliersNikita Trukhanov, Ilya SoloveychikPaperCherry on Top: Parameter Heterogeneity and Quantization in Large Language ModelsWanyun Cui, Qianle WangPaperIncreased LLM Vulnerabilities from Fine-tuning and QuantizationDivyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth HarshangiPaperQuantization of Large Language Models with an Overdetermined BasisDaniil Merkulov, Daria Cherniuk, Alexander Rudikov, Ivan Oseledets, Ekaterina Muravleva, Aleksandr Mikhalev, Boris KashinPaperdecoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating PointsYi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping Cai, Yang Zhang, Shouda LiuGithubPaperLossless and Near-Lossless Compression for Foundation ModelsMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, Danny HarnikPaperHow Good Are Low-bit Quantized LLaMA3 Models? An Empirical StudyWei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele MagnoGithubPaperModelWhen Quantization Affects Confidence of Large Language Models?Irina Proskurina, Luc Brun, Guillaume Metzler, Julien VelcinGithubPaperQServe: W4A8KV4 Quantization and System Co-design for Efficient LLM ServingYujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song HanGithubPaperInference AccelerationTitle & Authors\\nIntroduction\\nLinksDeja Vu: Contextual Sparsity for Efficient LLMs at Inference TimeZichang Liu, Jue WANG, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi ChenGithubPaperScissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test TimeZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali ShrivastavaPaperDynamic Context Pruning for Efficient and Interpretable Autoregressive TransformersSotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas HofmannPaperH2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language ModelsZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang, Beidi ChenGithubPaperLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperFast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel DecodingSangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young YunGithubPaperCompressing Context to Enhance Inference Efficiency of Large Language ModelsYucheng Li, Bo Dong, Chenghua Lin, Frank GuerinGithubPaperConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models InferenceZiqian Zeng, Yihuai Hong, Hongliang Dai, Huiping Zhuang, Cen ChenPaperAccelerating LLM Inference with Staged Speculative DecodingBenjamin Spector, Chris RePaperTCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost ReductionJunyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming QianPaperInference with Reference: Lossless Acceleration of Large Language ModelsNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, Furu WeiGithubpaperSpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree VerificationXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao JiaGithubpaperSkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM InferenceLuciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, Subhabrata MukherjeePaperSkeleton-of-Thought: Large Language Models Can Do Parallel DecodingXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu WangPaperDraft & Verify: Lossless Large Language Model Acceleration via Self-Speculative DecodingJun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad MehrotraGithubPaperEfficient Streaming Language Models with Attention SinksGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike LewisGithubPaper(Dynamic) Prompting might be all you need to repair Compressed LLMsDuc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang WangPaperModel Tells You What to Discard: Adaptive KV Cache Compression for LLMsSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng GaoPaperLarge Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient ReasoningMurong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu YaoGithubPaperLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt CompressionHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperCacheGen: Fast Context Loading for Language Model ApplicationsYuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen JiangPaperContext Compression for Auto-regressive Transformers with Sentinel TokensSiyu Ren, Qi Jia, Kenny Q. ZhuGithubPaperA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language ModelsShengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido ZucconGithubPaperSPEED: Speculative Pipelined Execution for Efficient DecodingColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia ShaoPaperAccelerating LLM Inference by Enabling Intermediate Layer DecodingNeeraj Varshney, Agneet Chatterjee, Mihir Parmar, Chitta BaralPaperFast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers FasterHongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai ChenPaperCompressed Context Memory For Online Language Model InteractionJang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh SongGithubPaperSparQ Attention: Bandwidth-Efficient LLM InferenceLuka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas OrrPaperLookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation AccuracyYao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie GuPaperCascade Speculative Drafting for Even Faster LLM InferenceZiyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan ChangPaperEAGLE: Lossless Acceleration of LLM Decoding by Feature ExtrapolationYuhui Li, Chao Zhang, and Hongyang ZhangGithubBlogLoMA: Lossless Compressed Memory AttentionYumeng Wang, Zhenyang XiaoPaperMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri DaoGithubPaperAPAR: LLMs Can Do Auto-Parallel Auto-Regressive DecodingMingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao DongPaperBiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language ModelsFeng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong XiaoGithubPaperGet More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM InferenceHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi ChenGithubPaperSpeculative Streaming: Fast LLM Inference without Auxiliary ModelsNikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar NajibiPaperRelayAttention for Efficient Large Language Model Serving with Long System PromptsLei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. LauPaperRecursive Speculative Decoding: Accelerating LLM Inference via Sampling Without ReplacementWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher LottPaperChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase PartitionLu Ye, Ze Tao, Yong Huang, Yang LiPaperChimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all TokensZiqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen ChenGithubPaperGEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLMHao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo ZhaoGithubPaperCHAI: Clustered Head Attention for Efficient LLM InferenceSaurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean WuPaperDynamic Memory Compression: Retrofitting LLMs for Accelerated InferencePiotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, Edoardo M. PontiPaperKeyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative InferenceMuhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham KamathPaperRecurrent Drafter for Fast Speculative Decoding in Large Language ModelsAonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei ChengPaperOptimal Block-Level Draft Verification for Accelerating Speculative DecodingZiteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha SureshPaperHierarchical Skip Decoding for Efficient Autoregressive Text GenerationYunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng ZhangPaperALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV CachingYoupeng Zhao, Di Wu, Jun WangPaperSDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive TokensChengbo Liu, Yong ZhuGithubPaperPrepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language ModelsSiyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya GroverGithubPaperTowards Fast Inference: Exploring and Improving Blockwise Parallel DraftsTaehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, Adrian BentonPaperLossless Acceleration of Large Language Model via Adaptive N-gram Parallel DecodingJie Ou, Yueming Chen, Wenhong TianGithubPaperSelf-Selected Attention Span for Accelerating Large Language Model InferenceTian Jin, Wanzin Yazar, Zifei Xu, Sayeh Sharify, Xin WangPaperParallel Decoding via Hidden Transfer for Lossless Large Language Model AccelerationPengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan ZhaoPaperXC-Cache: Cross-Attending to Cached Context for Efficient LLM InferenceJo√£o Monteiro, √âtienne Marcotte, Pierre-Andr√© No√´l, Valentina Zantedeschi, David V√°zquez, Nicolas Chapados, Christopher Pal, Perouz TaslakianPaperHybrid LLM: Cost-Efficient and Quality-Aware Query RoutingDujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks V.S. Lakshmanan, Ahmed Hassan AwadallahGithubPaperEfficient LLM Inference with KcacheQiaozhi He, Zhihua WuPaperBetter & Faster Large Language Models via Multi-token PredictionFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, Gabriel SynnaevePaperKV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache GenerationMinsik Cho, Mohammad Rastegari, Devang NaikPaperYou Only Cache Once: Decoder-Decoder Architectures for Language ModelsYutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu WeiGithubPaperKangaroo: Lossless Self-Speculative Decoding via Double Early ExitingFangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe WangGithubPaperAccelerating Speculative Decoding using Dynamic Speculation LengthJonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy SchwartzPaperClover: Regressive Lightweight Speculative Decoding with Sequential KnowledgeBin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin CuiPaperEfficient MOETitle & Authors\\nIntroduction\\nLinksSiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts ModelsZhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai \"Helen\" Li, Yiran ChenPaperFast Inference of Mixture-of-Experts Language Models with OffloadingArtyom Eliseev, Denis MazurGithubPaperSwitchHead: Accelerating Transformers with Mixture-of-Experts AttentionR√≥bert Csord√°s, Piotr Piƒôkos, Kazuki Irie, J√ºrgen SchmidhuberGithubPaperExploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model InferenceJinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK)PandaGithubPaperMoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE ServingLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh MarinaGithubPaperFiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts ModelsKeisuke Kamahori, Yile Gu, Kan Zhu, Baris KasikciGithubPaperNot All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language ModelsXudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng LiGithubPaperEnhancing Efficiency in Sparse Models with Sparser SelectionYuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, Zenglin XuGithubPaperPrompt-prompted Mixture of Experts for Efficient LLM GenerationHarry Dong, Beidi Chen, Yuejie ChiGithubPaperShortcut-connected Expert Parallelism for Accelerating Mixture-of-ExpertsWeilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi HuangPaperSEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-ExpertsAlexandre Muzio, Alex Sun, Churan HePaperDense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language ModelsBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar PandaPaperLancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication OverlappingChenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, Yida WangPaperEfficient Architecture of LLMTitle & Authors\\nIntroduction\\nLinksRethinking Optimization and Architecture for Tiny Language ModelsYehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe WangGithubPaperTandem Transformers for Inference Efficient LLMsAishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperScaling Efficient LLMsB.N. KausikPaperMobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use CasesZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas ChandraPaperThink Big, Generate Quick: LLM-to-SLM for Fast Autoregressive DecodingBenjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami BejnordiPaperMobiLlama: Towards Accurate and Lightweight Fully Transparent GPTOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz KhanGithubPaper ModelGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar GulcehrePaperDiJiang: Efficient Large Language Models through Compact KernelizationHanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe WangGithubPaperMegalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthXuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting ZhouGithubPaperHierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMsWoomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo ShinGithubPaperText CompressionTitle & Authors\\nIntroduction\\nLinksEntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text CompressionAlexander Tsvetkov. Alon KipnisPaperLLMZip: Lossless Text Compression using Large Language ModelsChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas ShakkottaiPaper | Unofficial GithubAdapting Language Models to Compress ContextsAlexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi ChenGithubPaperIn-context Autoencoder for Context Compression in a Large Language ModelTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu WeiPaperNugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language ModelGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van DurmePaperBoosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context PruningXijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Mao YangPaperProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel DecodingShuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru HuangPaperLearning to Compress Prompt in Natural Language FormatsYu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia HuPaperLLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt CompressionZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin et alPaperPCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language ModelsJinyi Li, Yihuai Lan, Lei Wang, Hao WangGithubPaperPROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt CompressionMuhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di WangPaperTraining LLMs over Neurally Compressed TextBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah ConstantPaperAdapting LLMs for Efficient Context Processing through Soft Prompt CompressionCangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian FloydPaperRethinking LLM Memorization through the Lens of Adversarial CompressionAvi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico KolterGithubPaperProjectLow-Rank DecompositionTitle & Authors\\nIntroduction\\nLinksLoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse ApproximationYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo ZhaoGithubPaperMatrix Compression via Randomized Low Rank and Low Precision FactorizationRajarshi Saha, Varun Srivastava, Mert PilanciGithubPaperTensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train DecompositionMingxue Xu, Yao Lei Xu, Danilo P. MandicPaperLORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot CompressionAyush Kaushal, Tejas Vaidhya, Irina RishPaperProjectRethinking Compression: Reduced Order Modelling of Latent Features in Large Language ModelsArnav Chavan, Nahush Lele, Deepak GuptaGithubPaperData-free Weight Compress and Denoise for Large Language ModelsRunyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua LinPaperSVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model CompressionXin Wang, Yu Zheng, Zhongwei Wan, Mi ZhangGithubPaperHardware/SystemFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Tri Dao, Daniel Y.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ",\n",
    "max_tokens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioFile = \"../Utils/AudioFiles/harvard.wav\"\n",
    "ff = open(audioFile, \"rb\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'readable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mff\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'readable'"
     ]
    }
   ],
   "source": [
    "type(io.BufferedReader(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---parameter generation completed---\n",
      "efficient LLM\n",
      "['https://github.com/horseee/Awesome-Efficient-LLM', 'https://arxiv.org/abs/2404.08801']\n",
      "[\"[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthSkip to main contentWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate> cs > arXiv:2404.08801Help | Advanced SearchAll fields\\nTitle\\nAuthor\\nAbstract\\nComments\\nJournal reference\\nACM classification\\nMSC classification\\nReport number\\narXiv identifier\\nDOI\\nORCID\\narXiv author ID\\nHelp pages\\nFull textSearchopen searchGOopen navigation menuquick linksLogin\\nHelp Pages\\nAboutComputer Science > Machine LearningarXiv:2404.08801 (cs)[Submitted on 12 Apr 2024 (v1), last revised 16 Apr 2024 (this version, v2)]\\nTitle:Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length\\nAuthors:Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authors\\nView PDF\\nHTML (experimental)Abstract:The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, a neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code: this https URLComments:\\n9 pages, 6 figures and 8 tablesSubjects:Machine Learning (cs.LG); Computation and Language (cs.CL)Cite as:\\narXiv:2404.08801 [cs.LG](orarXiv:2404.08801v2 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2404.08801Focus to learn morearXiv-issued DOI via DataCiteSubmission history From: Xuezhe Ma [view email][v1]Fri, 12 Apr 2024 20:28:14 UTC (568 KB)\\n[v2]Tue, 16 Apr 2024 07:27:58 UTC (572 KB)Full-text links:\\nAccess Paper:View a PDF of the paper titled Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, by Xuezhe Ma and 9 other authorsView PDFHTML (experimental)TeX SourceOther Formats\\nview licenseCurrent browse context: cs.LG<\\xa0prev|next\\xa0>new|recent|2404Change to browse by:cs\\ncs.CLReferences & CitationsNASA ADSGoogle Scholar\\nSemantic Scholara\\nexport BibTeX citation\\nLoading...BibTeX formatted citation\\n√óloading...Data provided by:BookmarkBibliographic ToolsBibliographic and Citation ToolsBibliographic Explorer ToggleBibliographic Explorer (What is the Explorer?)Litmaps ToggleLitmaps (What is Litmaps?)scite.ai Togglescite Smart Citations (What are Smart Citations?)Code, Data, MediaCode, Data and Media Associated with this ArticleLinks to Code ToggleCatalyzeX Code Finder for Papers (What is CatalyzeX?)DagsHub ToggleDagsHub (What is DagsHub?)GotitPub ToggleGotit.pub (What is GotitPub?)Links to Code TogglePapers with Code (What is Papers with Code?)ScienceCast ToggleScienceCast (What is ScienceCast?)DemosDemosReplicate ToggleReplicate (What is Replicate?)Spaces ToggleHugging Face Spaces (What is Spaces?)Spaces ToggleTXYZ.AI (What is TXYZ.AI?)Related PapersRecommenders and Search ToolsLink to Influence FlowerInfluence Flower (What are Influence Flowers?)Connected Papers ToggleConnected Papers (What is Connected Papers?)Core recommender toggleCORE Recommender (What is CORE?)IArxiv recommender toggleIArxiv Recommender\\n(What is IArxiv?)Author\\nVenue\\nInstitution\\nTopicAbout arXivLabsarXivLabs: experimental projects with community collaborators\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.Which authors of this paper are endorsers? |Disable MathJax (What is MathJax?)About\\nHelpcontact arXivClick here to contact arXivContactsubscribe to arXiv mailingsClick here to subscribeSubscribeCopyright\\nPrivacy PolicyWeb Accessibility AssistancearXiv Operational StatusGet status notifications viaemailor slack\", 'GitHub - horseee/Awesome-Efficient-LLM: A curated list for Efficient Large Language ModelsSkip to contentNavigation MenuToggle navigationSign inProductActionsAutomate any workflowPackagesHost and manage packagesSecurityFind and fix vulnerabilitiesCodespacesInstant dev environmentsCopilotWrite better code with AICode reviewManage code changesIssuesPlan and track workDiscussionsCollaborate outside of codeExploreAll featuresDocumentationGitHub SkillsBlogSolutionsForEnterpriseTeamsStartupsEducationBy SolutionCI/CD & AutomationDevOpsDevSecOpsResourcesLearning PathwaysWhite papers, Ebooks, WebinarsCustomer StoriesPartnersOpen SourceGitHub SponsorsFund open source developersThe ReadME ProjectGitHub community articlesRepositoriesTopicsTrendingCollectionsPricingSearch or jump to...Search code, repositories, users, issues, pull requests...SearchClearSearch syntax tipsProvide feedbackWe read every piece of feedback, and take your input very seriously.Include my email address so I can be contactedCancelSubmit feedbackSaved searchesUse saved searches to filter your results more quicklyNameQueryTo see all available qualifiers, see our documentation.CancelCreate saved searchSign inSign upYou signed in with another tab or window. Reload to refresh your session.\\nYou signed out in another tab or window. Reload to refresh your session.\\nYou switched accounts on another tab or window. Reload to refresh your session.Dismiss alerthorseee/Awesome-Efficient-LLMPublicNotificationsFork61Star824A curated list for Efficient Large Language Models824stars61forksBranchesTagsActivityStarNotificationsCodeIssues\\n0Pull requests\\n1ActionsProjects\\n0SecurityInsightsAdditional navigation optionsCodeIssuesPull requestsActionsProjectsSecurityInsightshorseee/Awesome-Efficient-LLMThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit\\xa0History323 Commitsefficient_plmefficient_plmfiguresfiguresprojectprojectREADME.mdREADME.mdgenerate_item.pygenerate_item.pyView all filesRepository files navigationREADMEAwesome-Efficient-LLM\\nA curated list for Efficient Large Language Models:Knowledge Distillation\\nNetwork Pruning\\nQuantization\\nInference Acceleration\\nEfficient MOE\\nEfficient Architecture of LLM\\nText Compression\\nLow-Rank Decomposition\\nHardware/System\\nTuning\\nSurvey\\nLeaderboardüöÄ UpdatesSep 27, 2023: Add tagfor papers accepted at NeurIPS\\'23.\\nSep 6, 2023: Add a new subdirectory project/ to organize those projects that are designed for developing a lightweight LLM.\\nJuly 11, 2023:\\nIn light of the numerous publications that conducts experiments using PLMs (such as BERT, BART) currently, a new subdirectory efficient_plm/ is created to house papers that are applicable to PLMs but have yet to be verified for their effectiveness on LLMs (not implying that they are not suitable on LLM).üíÆ Contributing\\nIf you\\'d like to include your paper, or need to update any details such as conference information or code URLs, please feel free to submit a pull request. You can generate the required markdown format for each paper by filling in the information in generate_item.py and execute python generate_item.py. We warmly appreciate your contributions to this list. Alternatively, you can email me with the links to your paper and code, and I would add your paper to the list at my earliest convenience.\\nKnowledge DistillationTitle & Authors\\nIntroduction\\nLinksSpecializing Smaller Language Models towards Multi-Step ReasoningYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar KhotGithubPaperDistilling Script Knowledge from Large Language Models for Constrained Language PlanningSiyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing YangGithubPaperSCOTT: Self-Consistent Chain-of-Thought DistillationPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang RenPaperDISCO: Distilling Counterfactuals with Large Language ModelsZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle RichardsonGithubPaperI2D2: Inductive Knowledge Distillation with NeuroLogic and Self-ImitationChandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin ChoiGithubPaperProjectSymbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-StepLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin ChoiGithubPaperCan Language Models Teach? Teacher Explanations Improve Student Performance via Theory of MindSwarnadeep Saha, Peter Hase, and Mohit BansalGithubPaperDialogue Chain-of-Thought Distillation for Commonsense-aware Conversational AgentsHyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, Jinyoung YeoPaperPromptMix: A Class Boundary Augmentation Method for Large Language Model DistillationGaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. LaradjiGithubPaperTurning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative DataYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan LiGithubPaperDemocratizing Reasoning Ability: Tailored Learning from Large Language ModelZhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi ZhangGithubPaperGKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language ModelShicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Yang Yang, Hongyin Tang, Keqing He, Jiahao Liu, Jingang Wang, Shu Zhao, Peng Zhang, Jie TangGithubPaperDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model SizesCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas PfisterGithubPaperRetrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model CompressionJiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Ran Lucien Wang, Rui YanPaperCache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language ModelsIlias Stogiannidis, Stavros Vassos, Prodromos Malakasiotis, Ion AndroutsopoulosGithubPaperEfficiently Distilling LLMs for Edge ApplicationsAchintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong, Rhui Dih LeePaperLaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham Fikri AjiGithub paperKnowledge Distillation of Large Language ModelsYuxian Gu, Li Dong, Furu Wei, Minlie HuangGithubPaperTeaching Small Language Models to ReasonLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn.PaperLarge Language Model Distillation Doesn\\'t Need a TeacherAnanya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz BeltagyGithub paperThe False Promise of Imitating Proprietary LLMsArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn SongPaperImpossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and ParaphrasingJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, Yejin ChoiGithub paperPaD: Program-aided Distillation Specializes Large Models in ReasoningXuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, Bowen ZhouPaperRLCD: Reinforcement Learning from Contrast Distillation for Language Model AlignmentKevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong TianPaperSci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QAYuhan Ma, Haiqi Jiang, Chenyou FanPaperUniversalNER: Targeted Distillation from Large Language Models for Open Named Entity RecognitionWenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung PoonGithubPaperProjectBaby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penaltyInar Timiryasov, Jean-Loup TastetGithubPaperDistillSpec: Improving Speculative Decoding via Knowledge DistillationYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran√ßois Kagy, Rishabh AgarwalPaperZephyr: Direct Distillation of LM AlignmentLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas WolfGithubPaperTowards the Law of Capacity Gap in Distilling Language ModelsChen Zhang, Dawei Song, Zheyu Ye, Yan GaoGithubPaperUnlock the Power: Competitive Distillation for Multi-Modal Large Language ModelsXinwei Li, Li Lin, Shuai Wang, Chen QianPaperMixed Distillation Helps Smaller Language Model Better ReasoningLi Chenglin, Chen Qianglong, Wang Caiyu, Zhang YinPaperDistilling Event Sequence Knowledge From Large Language ModelsSomin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker, Jian NiPaperKnowledge Distillation for Closed-Source Language ModelsHongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan, Ji ZhangPaperImproving Small Language Models\\' Mathematical Reasoning via Equation-of-Thought DistillationXunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping WangPaperScavenging Hyena: Distilling Transformers into Long Convolution ModelsTokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence LiangPaperDistiLLM: Towards Streamlined Distillation for Large Language ModelsJongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young YunGithubPaperLarge Language Model Meets Graph Neural Network in Knowledge DistillationShengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin ChenPaperUnmemorization in Large Language Models via Self-Distillation and Deliberate ImaginationYijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan VuliƒáGithubPaperTowards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMsNicolas Boizard, Kevin El-Haddad, C√©line Hudelot, Pierre ColomboGithub GithubPaperModelRevisiting Knowledge Distillation for Autoregressive Language ModelsQihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng TaoPaperPromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt TuningGyeongman Kim, Doohyuk Jang, Eunho YangPaperSelf-Distillation Bridges Distribution Gap in Language Model Fine-TuningZhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei ChenPaperWisdom of Committee: Distilling from Foundation Model to Specialized Application ModelZichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe ZhaoPaperDivide-or-Conquer? Which Part Should You Distill Your LLM?Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran, Navdeep Jaitly, Yizhe ZhangPaperDistillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and DistillationPhuc Phan, Hieu Tran, Long PhanGithubPaperLeveraging Zero-Shot Prompting for Efficient Language Model DistillationLukas V√∂ge, Vincent Gurgul, Stefan LessmannPaperMetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction TasksLetian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo ShangGithubPaperModelGecko: Versatile Text Embeddings Distilled from Large Language ModelsJinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer et alPaperRethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language ModelsTaiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai WongPaperBlog-Eng Blog-‰∏≠Post-Semantic-Thinking: A Robust Strategy to Distill Reasoning Capacity from Large Language ModelsXiaoshu Chen, Sihang Zhou, Ke Liang, Xinwang LiuPaperCompressing Long Context for Enhancing RAG with AMR-based Concept DistillationKaize Shi, Xueyao Sun, Qing Li, Guandong XuPaperNetwork PruningTitle & Authors\\nIntroduction\\nLinksSparseGPT: Massive Language Models Can Be Accurately Pruned in One-ShotElias Frantar, Dan AlistarhGithub paperLLM-Pruner: On the Structural Pruning of Large Language ModelsXinyin Ma, Gongfan Fang, Xinchao WangGithub paperThe Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that MatterAjay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang WangGithubPaperFlash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured SparsityHaojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon SongGithubPaperA Simple and Effective Pruning Approach for Large Language ModelsMingjie Sun, Zhuang Liu, Anna Bair, J. Zico KolterGithubPaperSheared LLaMA: Accelerating Language Model Pre-training via Structured PruningMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi ChenGithubPaperPlug-and-Play: An Efficient Post-training Pruning Method for Large Language ModelsYingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio CannistraciGithubPaperFluctuation-based Adaptive Structured Pruning for Large Language ModelsYongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao WangGithubPaperNASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language ModelsJongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young YunGithubPaperLoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-TuningMingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan ZhuangPaperPruning Large Language Models via Accuracy PredictorYupeng Ji, Yibo Cao, Jiucai LiuPaperCompressing LLMs: The Truth is Rarely Pure and Never SimpleAjay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei YangPaperJunk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through SparsityLu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang WangGithubPaperOutlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High SparsityLu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei LiuGithubPaperCompresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language ModelsSong Guo, Jiahang Xu, Li Lyna Zhang, Mao YangGithubPaperSparse Finetuning for Inference Acceleration of Large Language ModelsEldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan AlistarhGithubPaperReLU Strikes Back: Exploiting Activation Sparsity in Large Language ModelsIman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad FarajtabarPaperThe Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context LearningTian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan Ragan-Kelley, Gintare Karolina DziugaitePaperOne-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language ModelsHang Shao, Bei Liu, Yanmin QianPaperLoRAShear: Efficient Large Language Model Structured Pruning and Knowledge RecoveryTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming LiangGithubPaperDivergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantizationBj√∂rn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg, Patrick Schramowski, Matthias A√üenmacher, Kristian KerstingGithubPaperBeyond Size: How Gradients Shape Pruning Decisions in Large Language ModelsRocktim Jyoti Das, Liqun Ma, Zhiqiang ShenGithubPaperDynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMsYuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong JiGithubPaperE-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M SparsityYun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui KangPaperPERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMsMax Zimmer, Megi Andoni, Christoph Spiegel, Sebastian PokuttaGithubPaperFast and Optimal Weight Update for Pruned Large Language ModelsVladim√≠r Bo≈æaGithubPaperPruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-TuningAdib Hasan, Ileana Rugina, Alex WangGithubPaperSliceGPT: Compress Large Language Models by Deleting Rows and ColumnsSaleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James HensmanGithubPaperAPT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and InferenceBowen Zhao, Hannaneh Hajishirzi, Qingqing CaoPaperReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMsZhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong SunPaperEverybody Prune Now: Structured Pruning of LLMs with only Forward PassesLucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet TalwalkarGithubPaperAssessing the Brittleness of Safety Alignment via Pruning and Low-Rank ModificationsBoyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia et alGithubPaperProjectNutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language ModelsShengrui Li, Xueting Han, Jing BaiPaperLearn To be Efficient: Build Structured Sparsity in Large Language ModelsHaizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul PrakashPaperShortened LLaMA: A Simple Depth Pruning for Large Language ModelsBo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu SongGithubPaperSLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer BlocksJiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon KimGithubPaperHiRE: High Recall Approximate Top-k Estimation for Efficient LLM InferenceYashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperLaCo: Large Language Model Pruning via Layer CollapseYifei Yang, Zouying Cao, Hai ZhaoPaperProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language ModelsChenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li et alGithubPaper[Model-7B] [Model-13B]EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMsSong Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong JiGithubPaperBESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity AllocationPeng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping LuoGithubPaperShortGPT: Layers in Large Language Models are More Redundant Than You ExpectXin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, Weipeng ChenPaperEfficient Pruning of Large Language Model with Adaptive Estimation FusionJun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi WangPaperDecoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under CompressionJunyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie et alGithubPaperProjectCompressing Large Language Models by Streamlining the Unimportant LayerXiaodong Chen, Yuxuan Hu, Jing ZhangPaperMultilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language BehindHongchuan Zeng, Hongshen Xu, Lu Chen, Kai YuGithubPaperAccelerating Inference in Large Language Models with a Unified Layer Skipping StrategyYijin Liu, Fandong Meng, Jie ZhouGithubPaperLoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language ModelsGuangyan Li, Yongqiang Tang, Wensheng ZhangPaperCATS: Contextually-Aware Thresholding for Sparsity in Large Language ModelsJe-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, Azalia MirhoseiniPaperLayer Skip: Enabling Early Exit Inference and Self-Speculative DecodingMostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer et alPaperEnabling High-Sparsity Foundational Llama Models with Efficient Pretraining and DeploymentAbhinav Agarwalla, Abhay Gupta, Alexandre Marques, Shubhra Pandit, Michael Goin, Eldar Kurtic, Kevin Leong, Tuan Nguyen, Mahmoud Salem, Dan Alistarh, Sean Lie, Mark KurtzPaperDependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language ModelsZhiyu Guo, Hidetaka Kamigaito, Taro WanatnabePaperMixture-of-Depths: Dynamically allocating compute in transformer-based language modelsDavid Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, Adam SantoroPaperQuantizationTitle & Authors\\nIntroduction\\nLinksGPTQ: Accurate Post-Training Quantization for Generative Pre-trained TransformersElias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan AlistarhGithubPaperSmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language ModelsGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song HanGithubPaperQLoRA: Efficient Finetuning of Quantized LLMsTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke ZettlemoyerGithub PaperQuIP: 2-Bit Quantization of Large Language Models With GuaranteesJerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De SaXQGithubPaperMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer QuantizationJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo LeePaperQuantizable Transformers: Removing Outliers by Helping Attention Heads Do NothingYelysei Bondarenko, Markus Nagel, Tijmen BlankevoortGithub PaperLLM-FP4: 4-Bit Floating-Point Quantized TransformersShih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting ChengGithubPaperEnhancing Computation Efficiency in Large Language Models through Weight and Activation QuantizationJangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook ChoiPaperAgile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the EdgeXuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, Yanzhi WangPaperOmniQuant: Omnidirectionally Calibrated Quantization for Large Language ModelsWenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping LuoGithubPaperAffineQuant: Affine Transformation Quantization for Large Language ModelsYuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong JiGithubPaperGPT-Zip: Deep Compression of Finetuned Large Language ModelsBerivan Isik, Hermann Kumbong, Wanyi Ning, Xiaozhe Yao, Sanmi Koyejo, Ce ZhangPaperWatermarking LLMs with Weight QuantizationLinyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng QiuGithubPaperAWQ: Activation-aware Weight Quantization for LLM Compression and AccelerationJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song HanGithubPaperRPTQ: Reorder-based Post-training Quantization for Large Language ModelsZhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe WuGithub PaperZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank CompensationZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong HePaperSqueezeLLM: Dense-and-Sparse Quantization Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt KeutzerGithubPaperOutlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scalingXiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong LiuPaperInteger or Floating Point? New Outlooks for Low-Bit Quantization on Large Language ModelsYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi XuPaperLLM-QAT: Data-Free Quantization Aware Training for Large Language ModelsZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas ChandraPaperSpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight CompressionTim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan AlistarhGithubPaperOWQ: Lessons learned from activation outliers for weight quantization in large language modelsChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok ParkGithubPaperDo Emergent Abilities Exist in Quantized Large Language Models: An Empirical StudyPeiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong WenGithubPaperZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point FormatsXiaoxia Wu, Zhewei Yao, Yuxiong HePaperFPTQ: Fine-grained Post-Training Quantization for Large Language ModelsQingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, Yuchen XiePaperQuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive AlgorithmKayhan Behdin, Ayan Acharya, Aman Gupta, Qingquan Song, Siyu Zhu, Sathiya Keerthi, Rahul MazumderGithubPaperNorm Tweaking: High-performance Low-bit Quantization of Large Language ModelsLiang Li, Qingyuan Li, Bo Zhang, Xiangxiang ChuPaperOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMsWenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao LvGithubPaperQA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language ModelsYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, Qi TianGithubPaperModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular QuantizersJunjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr KuleshovPaperPB-LLM: Partially Binarized Large Language ModelsYuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen DongGithubPaperDual Grained Quantization: Efficient Fine-Grained Quantization for LLMLuoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou, Hong ZhouPaperQFT: Quantized Full-parameter Tuning of LLMs with Affordable ResourcesZhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt KeutzerPaperQLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language ModelsJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan ZhuangPaperLoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language ModelsYixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo ZhaoPaperTEQ: Trainable Equivalent Transformation for Quantization of LLMsWenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao ShenGithubPaperBitNet: Scaling 1-bit Transformers for Large Language ModelsHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu WeiPaperAtom: Low-bit Quantization for Efficient and Accurate LLM ServingYilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, Baris KasikciPaperAWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language ModelsBaisong Li, Xingwang Wang, Haixiao XuPaperAFPQ: Asymmetric Floating Point Quantization for LLMsYijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi XuGithubPaperA Speed Odyssey for Deployable Quantization of LLMsQingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen XiePaperLQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model FinetuningHan Guo, Philip Greengard, Eric P. Xing, Yoon KimGithubPaperEnabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and Asynchronous DequantizationJinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang, Guohao DaiPaperSmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLMJiayi Pan, Chengcan Wang, Kaifu Zheng, Yangguang Li, Zhenyu Wang, Bin FengGithubPaperZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative TasksXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei YaoGithubPaperExtreme Compression of Large Language Models via Additive QuantizationVage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan AlistarhGithubPaperFP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-DesignHaojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon SongPaperKVQuant: Towards 10 Million Context Length LLM Inference with KV Cache QuantizationColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir GholamiGithubPaperL4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQHyesung Jeon, Yulhwa Kim, Jae-joon KimPaperQuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice CodebooksAlbert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De SaGithubPaperBiLLM: Pushing the Limit of Post-Training Quantization for LLMsWei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan QiGithubPaperAccurate LoRA-Finetuning Quantization of LLMs via Information RetentionHaotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele MagnoGithubPaperApiQ: Finetuning of 2-Bit Quantized Large Language ModelBaohao Liao, Christof MonzPaperTowards Next-Level Post-Training Quantization of Hyper-Scale TransformersJunhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon JeonPaperEdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the EdgeXuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi WangGithubPaperBitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-DistillationDayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi XuGithubPaperWKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains MoreYuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang NiePaperDB-LLM: Accurate Dual-Binarization for Efficient LLMsHong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng TaoPaperOneBit: Towards Extremely Low-bit Large Language ModelsYuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang ChePaperBitDelta: Your Fine-Tune May Only Be Worth One BitJames Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle CaiGithubPaperAny-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMsYeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. LeePaperAPTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language ModelsZiyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao YuPaperGPTVQ: The Blessing of Dimensionality for LLM QuantizationMart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul WhatmoughGithubPaperA Comprehensive Evaluation of Quantization Strategies for Large Language ModelsRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi XiongPaperThe Era of 1-bit LLMs: All Large Language Models are in 1.58 BitsShuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu WeiPaperEvaluating Quantized Large Language ModelsShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu WangGithubPaperNo Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision QuantizationJune Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo LeePaperFlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor QuantizationYi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin PanPaperQAQ: Quality Adaptive Quantization for LLM KV CacheShichen Dong, Wen Cheng, Jiayu Qin, Wei WangGithubPaperWhat Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of PerturbationZhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui YanPaperFrameQuant: Flexible Low-Bit Quantization for TransformersHarshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas SinghPaperQuaRot: Outlier-Free 4-Bit Inference in Rotated LLMsSaleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, James HensmanGithubPaperAccurate Block Quantization in LLMs with OutliersNikita Trukhanov, Ilya SoloveychikPaperCherry on Top: Parameter Heterogeneity and Quantization in Large Language ModelsWanyun Cui, Qianle WangPaperIncreased LLM Vulnerabilities from Fine-tuning and QuantizationDivyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth HarshangiPaperQuantization of Large Language Models with an Overdetermined BasisDaniil Merkulov, Daria Cherniuk, Alexander Rudikov, Ivan Oseledets, Ekaterina Muravleva, Aleksandr Mikhalev, Boris KashinPaperdecoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating PointsYi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping Cai, Yang Zhang, Shouda LiuGithubPaperLossless and Near-Lossless Compression for Foundation ModelsMoshik Hershcovitch, Leshem Choshen, Andrew Wood, Ilias Enmouri, Peter Chin, Swaminathan Sundararaman, Danny HarnikPaperHow Good Are Low-bit Quantized LLaMA3 Models? An Empirical StudyWei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele MagnoGithubPaperModelWhen Quantization Affects Confidence of Large Language Models?Irina Proskurina, Luc Brun, Guillaume Metzler, Julien VelcinGithubPaperQServe: W4A8KV4 Quantization and System Co-design for Efficient LLM ServingYujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song HanGithubPaperInference AccelerationTitle & Authors\\nIntroduction\\nLinksDeja Vu: Contextual Sparsity for Efficient LLMs at Inference TimeZichang Liu, Jue WANG, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi ChenGithubPaperScissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test TimeZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali ShrivastavaPaperDynamic Context Pruning for Efficient and Interpretable Autoregressive TransformersSotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas HofmannPaperH2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language ModelsZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark Barrett, Zhangyang Wang, Beidi ChenGithubPaperLLMLingua: Compressing Prompts for Accelerated Inference of Large Language ModelsHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperFast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel DecodingSangmin Bae, Jongwoo Ko, Hwanjun Song, Se-Young YunGithubPaperCompressing Context to Enhance Inference Efficiency of Large Language ModelsYucheng Li, Bo Dong, Chenghua Lin, Frank GuerinGithubPaperConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models InferenceZiqian Zeng, Yihuai Hong, Hongliang Dai, Huiping Zhuang, Cen ChenPaperAccelerating LLM Inference with Staged Speculative DecodingBenjamin Spector, Chris RePaperTCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost ReductionJunyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, Yiming QianPaperInference with Reference: Lossless Acceleration of Large Language ModelsNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, Furu WeiGithubpaperSpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree VerificationXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao JiaGithubpaperSkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM InferenceLuciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah, Subhabrata MukherjeePaperSkeleton-of-Thought: Large Language Models Can Do Parallel DecodingXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, Yu WangPaperDraft & Verify: Lossless Large Language Model Acceleration via Self-Speculative DecodingJun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad MehrotraGithubPaperEfficient Streaming Language Models with Attention SinksGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike LewisGithubPaper(Dynamic) Prompting might be all you need to repair Compressed LLMsDuc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang WangPaperModel Tells You What to Discard: Adaptive KV Cache Compression for LLMsSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng GaoPaperLarge Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient ReasoningMurong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu YaoGithubPaperLongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt CompressionHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili QiuGithubPaperCacheGen: Fast Context Loading for Language Model ApplicationsYuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng, Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, Ganesh Ananthanarayanan, Junchen JiangPaperContext Compression for Auto-regressive Transformers with Sentinel TokensSiyu Ren, Qi Jia, Kenny Q. ZhuGithubPaperA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language ModelsShengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido ZucconGithubPaperSPEED: Speculative Pipelined Execution for Efficient DecodingColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia ShaoPaperAccelerating LLM Inference by Enabling Intermediate Layer DecodingNeeraj Varshney, Agneet Chatterjee, Mihir Parmar, Chitta BaralPaperFast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads to Answers FasterHongxuan Zhang, Zhining Liu, Jiaqi Zheng, Chenyi Zhuang, Jinjie Gu, Guihai ChenPaperCompressed Context Memory For Online Language Model InteractionJang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh SongGithubPaperSparQ Attention: Bandwidth-Efficient LLM InferenceLuka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas OrrPaperLookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation AccuracyYao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie GuPaperCascade Speculative Drafting for Even Faster LLM InferenceZiyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin Chen-Chuan ChangPaperEAGLE: Lossless Acceleration of LLM Decoding by Feature ExtrapolationYuhui Li, Chao Zhang, and Hongyang ZhangGithubBlogLoMA: Lossless Compressed Memory AttentionYumeng Wang, Zhenyang XiaoPaperMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri DaoGithubPaperAPAR: LLMs Can Do Auto-Parallel Auto-Regressive DecodingMingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, Yuxiao DongPaperBiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language ModelsFeng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong XiaoGithubPaperGet More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM InferenceHarry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi ChenGithubPaperSpeculative Streaming: Fast LLM Inference without Auxiliary ModelsNikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar NajibiPaperRelayAttention for Efficient Large Language Model Serving with Long System PromptsLei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. LauPaperRecursive Speculative Decoding: Accelerating LLM Inference via Sampling Without ReplacementWonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher LottPaperChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase PartitionLu Ye, Ze Tao, Yong Huang, Yang LiPaperChimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all TokensZiqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen ChenGithubPaperGEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLMHao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo ZhaoGithubPaperCHAI: Clustered Head Attention for Efficient LLM InferenceSaurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean WuPaperDynamic Memory Compression: Retrofitting LLMs for Accelerated InferencePiotr Nawrot, Adrian ≈Åa≈Ñcucki, Marcin Chochowski, David Tarjan, Edoardo M. PontiPaperKeyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative InferenceMuhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham KamathPaperRecurrent Drafter for Fast Speculative Decoding in Large Language ModelsAonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei ChengPaperOptimal Block-Level Draft Verification for Accelerating Speculative DecodingZiteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha SureshPaperHierarchical Skip Decoding for Efficient Autoregressive Text GenerationYunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng ZhangPaperALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV CachingYoupeng Zhao, Di Wu, Jun WangPaperSDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive TokensChengbo Liu, Yong ZhuGithubPaperPrepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language ModelsSiyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya GroverGithubPaperTowards Fast Inference: Exploring and Improving Blockwise Parallel DraftsTaehyeon Kim, Ananda Theertha Suresh, Kishore Papineni, Michael Riley, Sanjiv Kumar, Adrian BentonPaperLossless Acceleration of Large Language Model via Adaptive N-gram Parallel DecodingJie Ou, Yueming Chen, Wenhong TianGithubPaperSelf-Selected Attention Span for Accelerating Large Language Model InferenceTian Jin, Wanzin Yazar, Zifei Xu, Sayeh Sharify, Xin WangPaperParallel Decoding via Hidden Transfer for Lossless Large Language Model AccelerationPengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, Dongyan ZhaoPaperXC-Cache: Cross-Attending to Cached Context for Efficient LLM InferenceJo√£o Monteiro, √âtienne Marcotte, Pierre-Andr√© No√´l, Valentina Zantedeschi, David V√°zquez, Nicolas Chapados, Christopher Pal, Perouz TaslakianPaperHybrid LLM: Cost-Efficient and Quality-Aware Query RoutingDujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks V.S. Lakshmanan, Ahmed Hassan AwadallahGithubPaperEfficient LLM Inference with KcacheQiaozhi He, Zhihua WuPaperBetter & Faster Large Language Models via Multi-token PredictionFabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, Gabriel SynnaevePaperKV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache GenerationMinsik Cho, Mohammad Rastegari, Devang NaikPaperYou Only Cache Once: Decoder-Decoder Architectures for Language ModelsYutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, Furu WeiGithubPaperKangaroo: Lossless Self-Speculative Decoding via Double Early ExitingFangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Kai Han, Yunhe WangGithubPaperAccelerating Speculative Decoding using Dynamic Speculation LengthJonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy SchwartzPaperClover: Regressive Lightweight Speculative Decoding with Sequential KnowledgeBin Xiao, Chunan Shi, Xiaonan Nie, Fan Yang, Xiangwei Deng, Lei Su, Weipeng Chen, Bin CuiPaperEfficient MOETitle & Authors\\nIntroduction\\nLinksSiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts ModelsZhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai \"Helen\" Li, Yiran ChenPaperFast Inference of Mixture-of-Experts Language Models with OffloadingArtyom Eliseev, Denis MazurGithubPaperSwitchHead: Accelerating Transformers with Mixture-of-Experts AttentionR√≥bert Csord√°s, Piotr Piƒôkos, Kazuki Irie, J√ºrgen SchmidhuberGithubPaperExploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model InferenceJinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK)PandaGithubPaperMoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE ServingLeyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh MarinaGithubPaperFiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts ModelsKeisuke Kamahori, Yile Gu, Kan Zhu, Baris KasikciGithubPaperNot All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language ModelsXudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng LiGithubPaperEnhancing Efficiency in Sparse Models with Sparser SelectionYuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, Zenglin XuGithubPaperPrompt-prompted Mixture of Experts for Efficient LLM GenerationHarry Dong, Beidi Chen, Yuejie ChiGithubPaperShortcut-connected Expert Parallelism for Accelerating Mixture-of-ExpertsWeilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi HuangPaperSEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-ExpertsAlexandre Muzio, Alex Sun, Churan HePaperDense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language ModelsBowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude Oliva, Colin Raffel, Rameswar PandaPaperLancet: Accelerating Mixture-of-Experts Training via Whole Graph Computation-Communication OverlappingChenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, Yida WangPaperEfficient Architecture of LLMTitle & Authors\\nIntroduction\\nLinksRethinking Optimization and Architecture for Tiny Language ModelsYehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe WangGithubPaperTandem Transformers for Inference Efficient LLMsAishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth NetrapalliPaperScaling Efficient LLMsB.N. KausikPaperMobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use CasesZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas ChandraPaperThink Big, Generate Quick: LLM-to-SLM for Fast Autoregressive DecodingBenjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami BejnordiPaperMobiLlama: Towards Accurate and Lightweight Fully Transparent GPTOmkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz KhanGithubPaper ModelGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language ModelsSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar GulcehrePaperDiJiang: Efficient Large Language Models through Compact KernelizationHanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe WangGithubPaperMegalodon: Efficient LLM Pretraining and Inference with Unlimited Context LengthXuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting ZhouGithubPaperHierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMsWoomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo ShinGithubPaperText CompressionTitle & Authors\\nIntroduction\\nLinksEntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text CompressionAlexander Tsvetkov. Alon KipnisPaperLLMZip: Lossless Text Compression using Large Language ModelsChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas ShakkottaiPaper | Unofficial GithubAdapting Language Models to Compress ContextsAlexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi ChenGithubPaperIn-context Autoencoder for Context Compression in a Large Language ModelTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu WeiPaperNugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language ModelGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van DurmePaperBoosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context PruningXijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Mao YangPaperProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel DecodingShuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru HuangPaperLearning to Compress Prompt in Natural Language FormatsYu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia HuPaperLLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt CompressionZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin et alPaperPCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language ModelsJinyi Li, Yihuai Lan, Lei Wang, Hao WangGithubPaperPROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt CompressionMuhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di WangPaperTraining LLMs over Neurally Compressed TextBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah ConstantPaperAdapting LLMs for Efficient Context Processing through Soft Prompt CompressionCangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian FloydPaperRethinking LLM Memorization through the Lens of Adversarial CompressionAvi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico KolterGithubPaperProjectLow-Rank DecompositionTitle & Authors\\nIntroduction\\nLinksLoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse ApproximationYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo ZhaoGithubPaperMatrix Compression via Randomized Low Rank and Low Precision FactorizationRajarshi Saha, Varun Srivastava, Mert PilanciGithubPaperTensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train DecompositionMingxue Xu, Yao Lei Xu, Danilo P. MandicPaperLORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot CompressionAyush Kaushal, Tejas Vaidhya, Irina RishPaperProjectRethinking Compression: Reduced Order Modelling of Latent Features in Large Language ModelsArnav Chavan, Nahush Lele, Deepak GuptaGithubPaperData-free Weight Compress and Denoise for Large Language ModelsRunyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua LinPaperSVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model CompressionXin Wang, Yu Zheng, Zhongwei Wan, Mi ZhangGithubPaperHardware/SystemFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√©. [Paper][Github]FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. Tri Dao. [Paper][Github]Efficiently Scaling Transformer Inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean. [Paper]FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher R√©, Ion Stoica, Ce Zhang. [Paper][Github]Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica. [Paper][Github]Efficient LLM Inference on CPUs. Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng. [Paper][Github]\\nEdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, Mengwei Xu. [Paper]GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models. Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan Lin. [Paper]\\nRethinking Memory and Communication Cost for Efficient Large Language Model Training. Chan Wu, Hanxiao Zhang, Lin Ju, Jinjing Huang, Youshao Xiao, Zhaoxin Huan, Siyuan Li, Fanzhuang Meng, Lei Liang, Xiaolu Zhang, Jun Zhou. [Paper]\\nChameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models. Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso. [Paper]\\nFlashDecoding++: Faster Large Language Model Inference on GPUs. Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Hanyu Dong, Yu Wang. [Paper]Striped Attention: Faster Ring Attention for Causal Transformers. William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley. [Paper][Github]PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU. Yixin Song, Zeyu Mi, Haotong Xie, Haibo Chen. [Paper][Github]\\nLLM in a flash: Efficient Large Language Model Inference with Limited Memory. Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar. [Paper]\\nFlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGA. Shulin Zeng, Jun Liu, Guohao Dai, Xinhao Yang, Tianyu Fu, Hongyi Wang, Wenheng Ma, Hanbo Sun, Shiyao Li, Zixiao Huang, Yadong Dai, Jintao Li, Zehao Wang, Ruoyu Zhang, Kairui Wen, Xuefei Ning, Yu Wang. [Paper]\\nEfficient LLM inference solution on Intel GPU. Hui Wu, Yi Gan, Feng Yuan, Jing Ma, Wei Zhu, Yutao Xu, Hong Zhu, Yuhua Zhu, Xiaoli Liu, Jinghui Gu. [Paper][Github]Inferflow: an Efficient and Highly Configurable Inference Engine for Large Language Models. Shuming Shi, Enbo Zhao, Deng Cai, Leyang Cui, Xinting Huang, Huayang Li. [Paper][Github]DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, Yuxiong He. [Paper][Github]QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference. Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim. [Paper][Github]FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning. Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia. [Paper][Github]\\nBurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences. Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su. [Paper]Efficiently Programming Large Language Models using SGLang. Lianmin Zheng*, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, Ying Sheng*. [Paper] [Github]\\nMELTing point: Mobile Evaluation of Language Transformers. MELTing point: Mobile Evaluation of Language Transformers. [Paper]\\nDeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference. Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin. [Paper]\\nTransformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie. [Paper]\\nLoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism. Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, Xin Jin. [Paper]M√©lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity. Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica. [Paper][Github]\\nExpert Router: Orchestrating Efficient Language Model Inference through Prompt Classification. Josef Pichlmeier, Philipp Ross, Andre Luckow. [Paper]\\nEfficient and Economic Large Language Model Inference with Attention Offloading. Shaoyuan Chen, Yutong Lin, Mingxing Zhang, Yongwei Wu. [Paper]TuningCPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun. [Paper]ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models. Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo. [Paper][Github]\\nTRANSOM: An Efficient Fault-Tolerant System for Training LLMs. Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li. [Paper]\\nDEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. Devleena Das, Vivek Khetan. [Paper]LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models. Jianxin Yang. [Paper][Github]Sparse Fine-tuning for Inference Acceleration of Large Language Models. Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh. [Paper][Github][Github]ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization. Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal. [Paper][Github]\\nTowards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper. Chengyu Wang, Junbing Yan, Wei Zhang, Jun Huang. [Paper]SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification. Yuntao Gui, Xiao Yan, Peiqi Yin, Han Yang, James Cheng. [Paper][Github]LoRA+: Efficient Low Rank Adaptation of Large Models. Soufiane Hayou, Nikhil Ghosh, Bin Yu. [Paper][Github]\\nSparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning. Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You. [Paper]DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation. Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee. [Paper][Github]\\nLoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models. Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang. [Paper]\\nParameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey. Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun)Zhang, Sai Qian Zhang. [Paper]AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis. Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. [Paper][Github]BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. Qijun Luo, Hengxu Yu, Xiao Li. [Paper][Github]\\nIntuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning. Yijiang Liu, Rongyu Zhang, Huanrui Yang, Kurt Keutzer, Yuan Du, Li Du, Shanghang Zhang. [Paper]Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning. Jing Xu, Jingzhao Zhang. [Paper][Github]SurveyA Survey on Model Compression for Large Language Models. Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang. [Paper]The Efficiency Spectrum of Large Language Models: An Algorithmic Survey. Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang. [Paper][Github]Efficient Large Language Models: A Survey. Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang. [Paper][Github]\\nTowards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia. [Paper]Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao. [Paper][Github]A Survey of Resource-efficient LLM and Multimodal Foundation Models. Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu. [Paper][Github]\\nA Survey on Hardware Accelerators for Large Language Models. Christoforos Kachris. [Paper]Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, Yunxin Liu. [Paper][Github]\\nA Comprehensive Survey of Compression Algorithms for Language Models. Seungcheol Park, Jaehyeon Choi, Sojin Lee, U Kang. [Paper]\\nA Survey on Transformer Compression. Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao. [Paper]\\nModel Compression and Efficient Inference for Large Language Models: A Survey. Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He. [Paper]A Survey on Knowledge Distillation of Large Language Models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou. [Paper][Github]Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui. [Paper][Github][Blog]Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward. Arnav Chavan, Raghav Magazine, Shubham Kushwaha, M√©rouane Debbah, Deepak Gupta. [Paper][Github]\\nEfficient Prompting Methods for Large Language Models: A Survey. Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu. [Paper]\\nA Survey on Efficient Inference for Large Language Models. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, Yu Wang. [Paper]LeaderboardPlatform\\nAccessHuggingface LLM Perf Leaderboard\\n[Source]LLM Safety Leaderboard (for compressed models)}\\n[Source]LLMPerf Leaderboard\\n[Source]LLM API Hosts Leaderboard\\n[Source]ML.ENERGY Leaderboard\\n[Source]Models Leaderboard\\n[Source]Provider Leaderboard\\n[Source]AboutA curated list for Efficient Large Language ModelsTopicscompressionlanguage-modelknowledge-distillationmodel-quantizationpruning-algorithmsllmllm-compressionefficient-llmResourcesReadmeActivityStars824starsWatchers36watchingForks61forksReport repositoryReleasesNo releases publishedPackages0No packages publishedContributors8LanguagesPython\\n100.0%Footer¬© 2024 GitHub,\\xa0Inc.Footer navigationTermsPrivacySecurityStatusDocsContactManage cookiesDo not share my personal informationYou can‚Äôt perform that action at this time.']\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19251 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m audioFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Utils/AudioFiles/harvard.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m NoogleAI(audioFile)\n\u001b[1;32m----> 3\u001b[0m summarized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39msummarizeResults(driver\u001b[38;5;241m=\u001b[39mdriver)\n",
      "File \u001b[1;32md:\\DL_Learning\\NoogleAI\\codes\\noogleai.py:44\u001b[0m, in \u001b[0;36mNoogleAI.summarizeResults\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     42\u001b[0m text, sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m NoogleExtractor()\u001b[38;5;241m.\u001b[39mtextExtractor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvertSearchParams(), driver)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m---> 44\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarizeModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an AI assistant who can convert the website contents of multiple websites into a Layman term of reply. This reply should only contain the information which is very recent. Check the issued dates and remove irrelevant events or news. Make the summary with bullet points if can\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe website contents are given here: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---summarization completed---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent, sources\n",
      "File \u001b[1;32md:\\SOFTWARES\\INSTALLED\\Anaconda\\envs\\winTf\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SOFTWARES\\INSTALLED\\Anaconda\\envs\\winTf\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SOFTWARES\\INSTALLED\\Anaconda\\envs\\winTf\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\SOFTWARES\\INSTALLED\\Anaconda\\envs\\winTf\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SOFTWARES\\INSTALLED\\Anaconda\\envs\\winTf\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19251 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "audioFile = \"../Utils/AudioFiles/harvard.wav\"\n",
    "client = NoogleAI(audioFile)\n",
    "summarized = await client.summarizeResults(driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.save_screenshot(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to access external websites directly. However, if you can provide me with the specific information or topics you'd like to know about, I can help summarize and explain them for you in layman's terms.\n"
     ]
    }
   ],
   "source": [
    "print(summarized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(\"http://localhost:8888/hear\", files={\"audioForm\":open(\"../Utils/AudioFiles/harvard.wav\", \"rb\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The 4 Different Types of Workplace Friendships (Psychology Today)**\n",
      "\n",
      "- **Workplace Best Friend**: Close, platonic friendship with high trust and honesty.\n",
      "- **Workplace Close Friendly**: Good friends at work, might hang out outside work.\n",
      "- **Workplace Friendly**: Light friendship, mainly at work, little personal disclosure.\n",
      "- **Co-Worker Acquaintance**: Basic interaction, minimal personal connection.\n",
      "\n",
      "**Key Points:**\n",
      "- Good workplace relationships enhance innovation, job satisfaction, and engagement.\n",
      "- Different types of workplace friendships offer various benefits.\n",
      "- Emotional risks and challenges exist in maintaining workplace friendships.\n",
      "- Strategies for improving relationships with difficult coworkers include reframing perspectives and making efforts to be friendly.\n",
      "\n",
      "**We're Just 'Having Fun.' How To Be Friends With Benefits (Psych Central)**\n",
      "\n",
      "- **Definition**: Sexual relationship without emotional commitment.\n",
      "- **Challenges**: Hard to keep emotional and physical aspects separate due to hormonal influences.\n",
      "- **Who Might Find It Easier**: Aromantic individuals, those with certain mental health conditions, sex workers.\n",
      "- **When to Avoid**: Individuals with anxiety, rejection sensitivity, chronic depression, or a history of trauma.\n",
      "- **Tips**:\n",
      "  - Set sexual and non-sexual boundaries.\n",
      "  - Ensure mutual consent and clear communication.\n",
      "  - Agree on the relationship's duration and exclusivity.\n",
      "  - Regularly check in on each other's feelings.\n",
      "- **Challenges and Precautions**:\n",
      "  - Maintain sexual health by using barrier methods and regular STI testing.\n",
      "  - Navigate social situations and exclusivity agreements carefully.\n",
      "  - Be prepared for the potential development of unreciprocated feelings.\n",
      "  \n",
      "**Key Points:**\n",
      "- Friends with benefits relationships require clear boundaries and communication.\n",
      "- Emotional and physical health considerations are crucial.\n",
      "- Understand personal emotional limits and potential relationship challenges.\n",
      "- Awareness of both positive and negative outcomes is essential for managing expectations.\n"
     ]
    }
   ],
   "source": [
    "print(res.json()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdata = open(\"../Utils/AudioFiles/harvard.wav\", \"rb\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'RIFF\\xfc\\x961\\x00WAVEfmt \\x12\\x00\\x00\\x00\\x01\\x00\\x02\\x00D\\xac\\x00\\x00\\x10\\xb1\\x02\\x00\\x04\\x00\\x10\\x00\\x00\\x00data\\x90h1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\x00\\x00\\xff\\xff\\x00\\x00'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdata[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "bio = io.BytesIO(bdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "attribute 'name' of '_io.BufferedReader' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedReader(bio)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: attribute 'name' of '_io.BufferedReader' objects is not writable"
     ]
    }
   ],
   "source": [
    "file = io.BufferedReader(bio)\n",
    "file.name = \"audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='../Utils/AudioFiles/harvard.wav'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"../Utils/AudioFiles/harvard.wav\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedBufferedReader:\n",
    "    def __init__(self, reader: io.BufferedReader, name: str):\n",
    "        self.reader = reader\n",
    "        self.name = name\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        return self.reader.read(size)\n",
    "    \n",
    "    # Add other methods you might need from io.BufferedReader (e.g., close, readline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... inside your FastAPI endpoint:\n",
    "buffered_reader = io.BufferedReader(bio)\n",
    "named_reader = NamedBufferedReader(buffered_reader, \"recorded_audio.webm\")  # Or any desired name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NamedBufferedReader at 0x20bb20fe810>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"site:tfoco.com 40 year old lady Saudi retirement safe family office Bahrain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "/?FORM=Z9FD1\n",
      "None\n",
      "javascript:void(0);\n",
      "#\n",
      "javascript:void(0)\n",
      "/rewards/dashboard\n",
      "#\n",
      "javascript:void(0);\n",
      "/search?q=programming&setlang=hi&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=bn&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=ur&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=pa-guru&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=mr&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=te&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=ta&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=kn&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=gu&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=ml&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/search?q=programming&setlang=or&sid=24E6C8A535F861B20615DC7434D4606D\n",
      "/?scope=web&FORM=HDRSC1\n",
      "https://www.bing.com/ck/a?!&&p=0f32d34b6345c60aJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAyOA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L2ltYWdlcy9zZWFyY2g_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDMg&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=4aa06b9c9c5e1891JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAyOQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L3ZpZGVvcy9zZWFyY2g_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDMw&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=3c1d2b64e5ad4918JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzMA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L21hcHM_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDNA&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=de309c95aa266228JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzMQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L25ld3Mvc2VhcmNoP3E9cHJvZ3JhbW1pbmcmRk9STT1IRFJTQzY&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=22c3f7d4d292e146JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzMg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L3Nob3A_cT1wcm9ncmFtbWluZyZGT1JNPVNIT1BUQg&ntb=1\n",
      "javascript:void(0);\n",
      "https://www.bing.com/ck/a?!&&p=83d7d6d3f02a7648JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzMw&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPWZsaWdodHMmRk9STT1GQlNDT1A&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=b5fbd7b316344e4dJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzNA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPXRyYXZlbCZGT1JNPVRIU0NPUA&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=e5bf9d1e7d04554aJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzNQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPWhvdGVscyZGT1JNPUhUU0NPUA&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=744129079ff769b7JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTAzNg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1L2hvbWVzP0ZPUk09MDAwMDYw&ntb=1\n",
      "/bp/verify?FORM=000061\n",
      "javascript:void(0);\n",
      "javascript:void(0);\n",
      "javascript:\n",
      "https://www.bing.com/ck/a?!&&p=bd87b1eeaacb6186JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTEzMg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cucHJvZ3JhbWl6LmNvbS8&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=bd87b1eeaacb6186JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTEzMg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cucHJvZ3JhbWl6LmNvbS8&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=e232e997a7ba4f03JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE0OQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vYXJ0aWNsZS93aGF0LWlzLXByb2dyYW1taW5n&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=e232e997a7ba4f03JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE0OQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vYXJ0aWNsZS93aGF0LWlzLXByb2dyYW1taW5n&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=6f370bdf75289f6dJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE2Ng&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcHV0ZXJfcHJvZ3JhbW1pbmc&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=6f370bdf75289f6dJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE2Ng&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcHV0ZXJfcHJvZ3JhbW1pbmc&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=875f565b2abaf012JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE4NA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2FydGljbGVzL3doYXQtaXMtcHJvZ3JhbW1pbmc_bXNvY2tpZD0wMmQyNWY0N2JmMTk2NjEzMGU4YTRiOTZiZTM1Njc1MQ&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=875f565b2abaf012JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTE4NA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2FydGljbGVzL3doYXQtaXMtcHJvZ3JhbW1pbmc_bXNvY2tpZD0wMmQyNWY0N2JmMTk2NjEzMGU4YTRiOTZiZTM1Njc1MQ&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=b4c8b30307dc43a9JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIwMg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvcHJvZ3JhbW1pbmctdHV0b3JpYWwv&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=b4c8b30307dc43a9JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIwMg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvcHJvZ3JhbW1pbmctdHV0b3JpYWwv&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=e4cfce4ef6057364JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIxOQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vbGVhcm4vbGVhcm4taG93LXRvLWNvZGU&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=e4cfce4ef6057364JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIxOQ&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vbGVhcm4vbGVhcm4taG93LXRvLWNvZGU&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=db20449a9520d618JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIzNg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20v&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=db20449a9520d618JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTIzNg&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20v&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=bcd366e90c023012JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI1Mw&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cudzNzY2hvb2xzLmNvbS9weXRob24v&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=bcd366e90c023012JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI1Mw&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cudzNzY2hvb2xzLmNvbS9weXRob24v&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=1dc1cf96001a87f2JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI3MA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cua2hhbmFjYWRlbXkub3JnL2NvbXB1dGluZy9jb21wdXRlci1wcm9ncmFtbWluZw&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=1dc1cf96001a87f2JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI3MA&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cua2hhbmFjYWRlbXkub3JnL2NvbXB1dGluZy9jb21wdXRlci1wcm9ncmFtbWluZw&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=febe11570505b646JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI4Nw&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL3Byb2dyYW1taW5nLWZ1bmRhbWVudGFscz9tc29ja2lkPTAyZDI1ZjQ3YmYxOTY2MTMwZThhNGI5NmJlMzU2NzUx&ntb=1\n",
      "https://www.bing.com/ck/a?!&&p=febe11570505b646JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0wMmQyNWY0Ny1iZjE5LTY2MTMtMGU4YS00Yjk2YmUzNTY3NTEmaW5zaWQ9NTI4Nw&ptn=3&ver=2&hsh=3&fclid=02d25f47-bf19-6613-0e8a-4b96be356751&u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL3Byb2dyYW1taW5nLWZ1bmRhbWVudGFscz9tc29ja2lkPTAyZDI1ZjQ3YmYxOTY2MTMwZThhNGI5NmJlMzU2NzUx&ntb=1\n",
      "None\n",
      "None\n",
      "/search?q=programming&FPIG=9BC104766C5544F49D1AA0BA5F36E8EF&first=11&FORM=PERE\n",
      "/search?q=programming&FPIG=9BC104766C5544F49D1AA0BA5F36E8EF&first=21&FORM=PERE1\n",
      "/search?q=programming&FPIG=9BC104766C5544F49D1AA0BA5F36E8EF&first=31&FORM=PERE2\n",
      "/search?q=programming&FPIG=9BC104766C5544F49D1AA0BA5F36E8EF&first=11&FORM=PORE\n",
      "javascript:void(0);\n",
      "/search?q=programming&FORM=000017&qpvt=programming\n",
      "/search?q=programming&filters=ex1%3a%22ez1%22&FORM=000017\n",
      "/search?q=programming&filters=ex1%3a%22ez2%22&FORM=000017\n",
      "/search?q=programming&filters=ex1%3a%22ez3%22&FORM=000017\n",
      "/search?q=programming&filters=ex1%3a%22ez5_19574_19939%22&FORM=000017\n"
     ]
    }
   ],
   "source": [
    "# Import required modules.\n",
    "from azure.cognitiveservices.search.websearch import WebSearchClient\n",
    "from azure.cognitiveservices.search.websearch.models import SafeSearch\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Replace with your subscription key.\n",
    "subscription_key = \"YOUR_SUBSCRIPTION_KEY\"\n",
    "\n",
    "# Instantiate the client and replace with your endpoint.\n",
    "client = WebSearchClient(endpoint=\"YOUR_ENDPOINT\", credentials=CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "# Make a request. Replace Yosemite if you'd like.\n",
    "web_data = client.web.search(query=\"Yosemite\")\n",
    "print(\"\\r\\nSearched for Query# \\\" Yosemite \\\"\")\n",
    "\n",
    "'''\n",
    "Web pages\n",
    "If the search response contains web pages, the first result's name and url\n",
    "are printed.\n",
    "'''\n",
    "if hasattr(web_data.web_pages, 'value'):\n",
    "\n",
    "    print(\"\\r\\nWebpage Results#{}\".format(len(web_data.web_pages.value)))\n",
    "\n",
    "    first_web_page = web_data.web_pages.value[0]\n",
    "    print(\"First web page name: {} \".format(first_web_page.name))\n",
    "    print(\"First web page URL: {} \".format(first_web_page.url))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't find any web pages...\")\n",
    "\n",
    "'''\n",
    "Images\n",
    "If the search response contains images, the first result's name and url\n",
    "are printed.\n",
    "'''\n",
    "if hasattr(web_data.images, 'value'):\n",
    "\n",
    "    print(\"\\r\\nImage Results#{}\".format(len(web_data.images.value)))\n",
    "\n",
    "    first_image = web_data.images.value[0]\n",
    "    print(\"First Image name: {} \".format(first_image.name))\n",
    "    print(\"First Image URL: {} \".format(first_image.url))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't find any images...\")\n",
    "\n",
    "'''\n",
    "News\n",
    "If the search response contains news, the first result's name and url\n",
    "are printed.\n",
    "'''\n",
    "if hasattr(web_data.news, 'value'):\n",
    "\n",
    "    print(\"\\r\\nNews Results#{}\".format(len(web_data.news.value)))\n",
    "\n",
    "    first_news = web_data.news.value[0]\n",
    "    print(\"First News name: {} \".format(first_news.name))\n",
    "    print(\"First News URL: {} \".format(first_news.url))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't find any news...\")\n",
    "\n",
    "'''\n",
    "If the search response contains videos, the first result's name and url\n",
    "are printed.\n",
    "'''\n",
    "if hasattr(web_data.videos, 'value'):\n",
    "\n",
    "    print(\"\\r\\nVideos Results#{}\".format(len(web_data.videos.value)))\n",
    "\n",
    "    first_video = web_data.videos.value[0]\n",
    "    print(\"First Videos name: {} \".format(first_video.name))\n",
    "    print(\"First Videos URL: {} \".format(first_video.url))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't find any videos...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a data-priority=\"2\" href=\"#\" id=\"b_skip_to_content\" role=\"button\" tabindex=\"0\"><div class=\"text-back\"><div class=\"text\" href=\"#\">Skip to content</div></div></a>,\n",
       " <a class=\"b_logoArea\" h=\"ID=SERP,5053.1\" href=\"/?FORM=Z9FD1\" target=\"\"><h1 class=\"b_logo\" title=\"Back to Bing search\"></h1></a>,\n",
       " <a aria-label=\"Clear text\" data-second-href=\"javascript:void(0)\" id=\"sw_clx\" role=\"button\" tabindex=\"0\"><div class=\"sw_close\"></div></a>,\n",
       " <a aria-label=\"Search button\" h=\"ID=SERP,5054.1\" href=\"javascript:void(0);\" tabindex=\"-1\"><div data-sbtip=\"Search the web\" id=\"sb_go_par\"><div class=\"sb_icon\" id=\"b_icon_spyglass\"></div><input class=\"b_searchboxSubmit\" id=\"sb_form_go\" name=\"go\" tabindex=\"0\" type=\"submit\" value=\"Search\"/></div></a>,\n",
       " <a aria-controls=\"id_d\" aria-haspopup=\"true\" class=\"id_button\" data-clarity-mask=\"true\" h=\"ID=SERP,5060.1\" href=\"#\" id=\"id_l\"><div> <span class=\"cbtn\" data-appns=\"SERP\" data-k=\"5096.1\" data-wire=\"I;button_init;; |\" id=\"6025A2_1_btn\"><input aria-label=\"\" id=\"id_a\" name=\"submit\" type=\"submit\" value=\"Sign in\"/></span></div><span aria-hidden=\"true\" id=\"id_n\" style=\"display:none\"></span><img alt=\"Profile Picture\" class=\"id_avatar sw_spd\" data-alt=\"\" id=\"id_p\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNgYAAAAAMAASsJTYQAAAAASUVORK5CYII=\" style=\"display:none\"/><div id=\"id_linkicon\" style=\"position:relative;vertical-align:top;margin-right:-10px;right:10px;display:none\"><svg class=\"id_linkicon_svg\" fill=\"none\" height=\"16\" style=\"width:16px;height:16px;vertical-align:top;margin-top:27px;\" viewbox=\"0 0 20 20\" width=\"16\" xmlns=\"http://www.w3.org/2000/svg\"><g filter=\"url(#a)\"><circle cx=\"10\" cy=\"9\" fill=\"#fff\" r=\"8\"></circle></g><path d=\"M8.2 6a.6.6 0 0 1 .07 1.196L8.2 7.2H7a1.8 1.8 0 0 0-.106 3.597L7 10.8h1.2a.6.6 0 0 1 .07 1.196L8.2 12H7a3 3 0 0 1-.13-5.997L7 6h1.2ZM13 6a3 3 0 0 1 .13 5.997L13 12h-1.2a.6.6 0 0 1-.07-1.196l.07-.004H13a1.8 1.8 0 0 0 .106-3.597L13 7.2h-1.2a.6.6 0 0 1-.07-1.196L11.8 6H13ZM7 8.4h6a.6.6 0 0 1 .07 1.196L13 9.6H7a.6.6 0 0 1-.07-1.196L7 8.4h6-6Z\" fill=\"#767676\"></path><defs><filter color-interpolation-filters=\"sRGB\" filterunits=\"userSpaceOnUse\" height=\"20\" id=\"a\" width=\"20\" x=\"0\" y=\"0\"><feflood flood-opacity=\"0\" result=\"BackgroundImageFix\"></feflood><fecolormatrix in=\"SourceAlpha\" result=\"hardAlpha\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0\"></fecolormatrix><feoffset dy=\"1\"></feoffset><fegaussianblur stddeviation=\"1\"></fegaussianblur><fecomposite in2=\"hardAlpha\" operator=\"out\"></fecomposite><fecolormatrix values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.14 0\"></fecolormatrix><feblend in2=\"BackgroundImageFix\" result=\"effect1_dropShadow_1352_97902\"></feblend><feblend in=\"SourceGraphic\" in2=\"effect1_dropShadow_1352_97902\" result=\"shape\"></feblend></filter></defs></svg></div></a>,\n",
       " <a aria-controls=\"bepfo\" aria-expanded=\"false\" aria-haspopup=\"true\" aria-label=\"Microsoft Rewards \" class=\"id_button toolTip\" h=\"ID=SERP,5068.1\" href=\"javascript:void(0)\" id=\"id_rh\" role=\"button\"><span class=\"serp\" data-priority=\"\" id=\"id_rc\">Rewards</span><span class=\"rwds_svg serp pc\" id=\"rewards_header_icon serp\"><span class=\"rhlined serp\"></span><span class=\"rhfill serp\"></span><svg id=\"rh_meter\" xmlns=\"http://www.w3.org/2000/svg\"><circle class=\"serp\" cx=\"20\" cy=\"20\" id=\"rh_animcrcl\" r=\"14\" stroke-dasharray=\"88, 88\" transform=\"rotate(-90,20,20)\"></circle></svg></span></a>,\n",
       " <a class=\"b_hide\" data-title=\"Microsoft Rewards\" h=\"ID=SERP,5067.1\" href=\"/rewards/dashboard\" id=\"id_rwl\"></a>,\n",
       " <a aria-controls=\"id_hbfo\" aria-expanded=\"false\" aria-haspopup=\"true\" aria-label=\"Settings and quick links\" class=\"idp_ham nohphbtop\" h=\"ID=SERP,5077.1\" href=\"#\" id=\"id_sc\" role=\"button\" tabindex=\"0\"></a>,\n",
       " <a aria-controls=\"langDId\" aria-current=\"page\" aria-expanded=\"false\" aria-haspopup=\"true\" h=\"ID=SERP,5039.1\" href=\"javascript:void(0);\" id=\"langChangeAnchor\" role=\"button\"><span id=\"langLid\">English</span><span class=\"sw_ddbk\" id=\"langChev\"></span></a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5041.1\" href=\"/search?q=programming&amp;setlang=hi&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡§π‡§ø‡§Ç‡§¶‡•Ä</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5042.1\" href=\"/search?q=programming&amp;setlang=bn&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5043.1\" href=\"/search?q=programming&amp;setlang=ur&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">ÿßÿ±ÿØŸà</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5044.1\" href=\"/search?q=programming&amp;setlang=pa-guru&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5045.1\" href=\"/search?q=programming&amp;setlang=mr&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡§Æ‡§∞‡§æ‡§†‡•Ä</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5046.1\" href=\"/search?q=programming&amp;setlang=te&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5047.1\" href=\"/search?q=programming&amp;setlang=ta&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5048.1\" href=\"/search?q=programming&amp;setlang=kn&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡≤ï‡≤®‡≥ç‡≤®‡≤°</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5049.1\" href=\"/search?q=programming&amp;setlang=gu&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5050.1\" href=\"/search?q=programming&amp;setlang=ml&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</a>,\n",
       " <a data-noajax=\"1\" h=\"ID=SERP,5051.1\" href=\"/search?q=programming&amp;setlang=or&amp;sid=153CBF85F3AE63D73437AB54F22562D9\" role=\"menuitem\">‡¨ì‡≠ú‡¨ø‡¨Ü</a>,\n",
       " <a aria-current=\"page\" class=\"\" h=\"ID=SERP,5027.1\" href=\"/?scope=web&amp;FORM=HDRSC1\">All</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5028.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=7025b2cb9c5251e5JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAyOA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L2ltYWdlcy9zZWFyY2g_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDMg&amp;ntb=1\" target=\"_blank\">Images</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5029.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=d778a265a47775c9JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAyOQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L3ZpZGVvcy9zZWFyY2g_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDMw&amp;ntb=1\" target=\"_blank\">Videos</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5030.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=6098683ced230d16JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzMA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L21hcHM_cT1wcm9ncmFtbWluZyZGT1JNPUhEUlNDNA&amp;ntb=1\" target=\"_blank\">Maps</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5031.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=2fbdcf47d53b01fdJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzMQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L25ld3Mvc2VhcmNoP3E9cHJvZ3JhbW1pbmcmRk9STT1IRFJTQzY&amp;ntb=1\" target=\"_blank\">News</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5032.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=d30e0f58073905cfJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzMg&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L3Nob3A_cT1wcm9ncmFtbWluZyZGT1JNPVNIT1BUQg&amp;ntb=1\" target=\"_blank\">Shopping</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5033.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=c2b05427a170b678JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzMw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPWZsaWdodHMmRk9STT1GQlNDT1A&amp;ntb=1\" target=\"_blank\">Flights</a>,\n",
       " <a aria-current=\"false\" aria-haspopup=\"true\" aria-label=\"Dropdown Menu\" href=\"javascript:void(0);\" role=\"button\" tabindex=\"0\" target=\"_self\"><span class=\"b_sp_menu_separ\" id=\"b_sp_menu_separ\"><svg focusable=\"false\" viewbox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z\"></path></svg></span>More</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5034.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=aab40b1d44051b00JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzNA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPXRyYXZlbCZGT1JNPVRIU0NPUA&amp;ntb=1\" target=\"_blank\">Travel</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5035.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=25f586a2e5934a08JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzNQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L3RyYXZlbC9zZWFyY2g_cT1wcm9ncmFtbWluZyZtPWhvdGVscyZGT1JNPUhUU0NPUA&amp;ntb=1\" target=\"_blank\">Hotels</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5036.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=4f2c53b1c5d0e3f0JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzNg&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L2hvbWVzP0ZPUk09MDAwMDYw&amp;ntb=1\" target=\"_blank\">Real Estate</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5037.1\" href=\"/bp/verify?FORM=000061\">My Bing</a>,\n",
       " <a aria-current=\"false\" class=\"\" h=\"ID=SERP,5038.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=d04897cca714ab13JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTAzOA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1L2hlYWx0aC9mZWVkP3E9cHJvZ3JhbW1pbmcmZmVhdHVyZXM9YmluZ2hmJkZPUk09SEZFU0NP&amp;ntb=1\" target=\"_blank\">Health</a>,\n",
       " <a h=\"ID=SERP,5059.1\" href=\"javascript:void(0);\"><svg fill=\"none\" height=\"16\" width=\"12\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M3 .5a.5.5 0 0 0-1 0V1h-.5A1.5 1.5 0 0 0 0 2.5v12A1.5 1.5 0 0 0 1.5 16H7v-3.5A1.5 1.5 0 0 1 8.5 11H12V2.5A1.5 1.5 0 0 0 10.5 1H10V.5a.5.5 0 0 0-1 0V1H6.5V.5a.5.5 0 0 0-1 0V1H3zm0 5a.5.5 0 0 1 .5-.5h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1-.5-.5M3.5 8h5a.5.5 0 0 1 0 1h-5a.5.5 0 0 1 0-1M3 11.5a.5.5 0 0 1 .5-.5h2a.5.5 0 0 1 0 1h-2a.5.5 0 0 1-.5-.5m8.69.5H8.5a.5.5 0 0 0-.5.5v3.19q.077-.06.146-.13l3.415-3.414q.07-.07.128-.146\"></path></svg>Notebook</a>,\n",
       " <a h=\"ID=SERP,5058.1\" href=\"javascript:void(0);\">Tools</a>,\n",
       " <a aria-controls=\"d20095\" aria-expanded=\"false\" aria-haspopup=\"true\" aria-label=\"Filtered by Date\" class=\"ftrH\" h=\"ID=SERP,5355.1\" href=\"javascript:\" id=\"h20095\" role=\"button\"><span class=\"fs_label\">Date</span><span class=\"sw_ddbk\"></span> </a>,\n",
       " <a aria-label=\"programiz.com\" class=\"tilk\" h=\"ID=SERP,5133.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=462be2a0726b2549JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTEzMw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cucHJvZ3JhbWl6LmNvbS8&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=90&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">programiz.com</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"0|5099|4764188612497765|adjKFxq40nJ2gKx-kkwVdyVHgSH_xF4l\"><cite>https://www.programiz.com</cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5134.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5133.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=462be2a0726b2549JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTEzMw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cucHJvZ3JhbWl6LmNvbS8&amp;ntb=1\" target=\"_blank\">Programiz: Learn to Code for Free</a>,\n",
       " <a aria-label=\"codecademy.com\" class=\"tilk\" h=\"ID=SERP,5150.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=5d82347a9dd5d198JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE1MA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vYXJ0aWNsZS93aGF0LWlzLXByb2dyYW1taW5n&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=91&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">codecademy.com</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"1|5100|5000785479139427|JcviMYSZPqviGAM1m-K3m3mfXTakl2ka\"><cite>https://<strong>www.codecademy.com</strong>/article/<strong>what-is-programming</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5151.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5150.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=5d82347a9dd5d198JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE1MA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vYXJ0aWNsZS93aGF0LWlzLXByb2dyYW1taW5n&amp;ntb=1\" target=\"_blank\"><strong>What is Programming</strong>? | <strong>Codecademy</strong></a>,\n",
       " <a aria-label=\"wikipedia.org\" class=\"tilk\" h=\"ID=SERP,5167.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=a0276bb0de6858aaJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE2Nw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcHV0ZXJfcHJvZ3JhbW1pbmc&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=92&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">wikipedia.org</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"2|5101|4574535743007814|qLdS9xc7Qo4gQkaagamWm8bl2frWg-kW\"><cite>https://<strong>en.wikipedia.org</strong>/wiki/<strong>Computer_programming</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5168.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5167.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=a0276bb0de6858aaJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE2Nw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcHV0ZXJfcHJvZ3JhbW1pbmc&amp;ntb=1\" target=\"_blank\"><strong>Computer programming</strong> - <strong>Wikipedia</strong></a>,\n",
       " <a aria-label=\"coursera.org\" class=\"tilk\" h=\"ID=SERP,5185.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=bdf7a2fdb5a2805bJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE4NQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2FydGljbGVzL3doYXQtaXMtcHJvZ3JhbW1pbmc_bXNvY2tpZD0zMDcyYmI5OGI4NDk2NDE0MzA4MWFmNDliOWMyNjVjNw&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=93&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">coursera.org</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"3|5102|4996224221662751|IUw0Hc22pIvgBW3rurAbyGu8jx9mi1fr\"><cite>https://<strong>www.coursera.org</strong>/articles/what-is-<strong>programming</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5186.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5185.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=bdf7a2fdb5a2805bJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTE4NQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2FydGljbGVzL3doYXQtaXMtcHJvZ3JhbW1pbmc_bXNvY2tpZD0zMDcyYmI5OGI4NDk2NDE0MzA4MWFmNDliOWMyNjVjNw&amp;ntb=1\" target=\"_blank\">What Is <strong>Programming</strong>? And How To Get Started | <strong>Coursera</strong></a>,\n",
       " <a aria-label=\"geeksforgeeks.org\" class=\"tilk\" h=\"ID=SERP,5203.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=fb4e40a6a2160fd9JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIwMw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvcHJvZ3JhbW1pbmctdHV0b3JpYWwv&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=94&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">geeksforgeeks.org</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"4|5103|4823244412887881|lRx3dtuPgpmRW7Ljh9Ga-7x4uLDGU73Y\"><cite>https://www.geeksforgeeks.org/<strong>programming</strong>-tutorial</cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5204.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5203.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=fb4e40a6a2160fd9JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIwMw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuZ2Vla3Nmb3JnZWVrcy5vcmcvcHJvZ3JhbW1pbmctdHV0b3JpYWwv&amp;ntb=1\" target=\"_blank\"><strong>Programming</strong> Tutorial | Concepts, Getting started, Roadmap, ‚Ä¶</a>,\n",
       " <a aria-label=\"codecademy.com\" class=\"tilk\" h=\"ID=SERP,5220.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=9e5f8ab07294184cJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIyMA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vbGVhcm4vbGVhcm4taG93LXRvLWNvZGU&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=95&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">codecademy.com</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"5|5104|4586815049252691|3fshYlGwv_Ql17MMo-0DUDci4QLUrt6K\"><cite>https://<strong>www.codecademy.com</strong>/learn/<strong>learn-how-to-code</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5221.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5220.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=9e5f8ab07294184cJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIyMA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20vbGVhcm4vbGVhcm4taG93LXRvLWNvZGU&amp;ntb=1\" target=\"_blank\"><strong>Learn How to Code</strong> | <strong>Codecademy</strong></a>,\n",
       " <a aria-label=\"codecademy.com\" class=\"tilk\" h=\"ID=SERP,5237.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=7f6d3b2b4c251c8bJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIzNw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20v&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=96&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">codecademy.com</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"6|5105|4895189407255239|tBcFDjuuvdmyEzLdlDsXWF0c-EX3jVTr\"><cite>https://<strong>www.codecademy.com</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5238.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5237.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=7f6d3b2b4c251c8bJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTIzNw&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY29kZWNhZGVteS5jb20v&amp;ntb=1\" target=\"_blank\">Learn to Code - for Free | <strong>Codecademy</strong></a>,\n",
       " <a aria-label=\"w3schools.com\" class=\"tilk\" h=\"ID=SERP,5254.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=91f6e569790c65f8JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI1NA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cudzNzY2hvb2xzLmNvbS9weXRob24v&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=97&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">w3schools.com</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"7|5106|5049159694423113|i_W6zx74BxX4F8LUvMc5s4SWheoP62uC\"><cite>https://<strong>www.w3schools.com</strong>/python</cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5255.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5254.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=91f6e569790c65f8JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI1NA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cudzNzY2hvb2xzLmNvbS9weXRob24v&amp;ntb=1\" target=\"_blank\"><strong>Python Tutorial</strong> - <strong>W3Schools</strong></a>,\n",
       " <a aria-label=\"khanacademy.org\" class=\"tilk\" h=\"ID=SERP,5271.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=e77f3d380c4f1a20JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI3MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cua2hhbmFjYWRlbXkub3JnL2NvbXB1dGluZy9jb21wdXRlci1wcm9ncmFtbWluZw&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=98&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">khanacademy.org</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"8|5107|4604007806483776|rSW1OFT7bS8tqS5EixiokwnAxKxdtBRt\"><cite>https://<strong>www.khanacademy.org</strong>/computing/<strong>computer-programming</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5272.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5271.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=e77f3d380c4f1a20JmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI3MQ&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cua2hhbmFjYWRlbXkub3JnL2NvbXB1dGluZy9jb21wdXRlci1wcm9ncmFtbWluZw&amp;ntb=1\" target=\"_blank\"><strong>Computer programming</strong> - JavaScript and the web - <strong>Khan Academy</strong></a>,\n",
       " <a aria-label=\"coursera.org\" class=\"tilk\" h=\"ID=SERP,5288.1\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=a188e7180426a08cJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI4OA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL3Byb2dyYW1taW5nLWZ1bmRhbWVudGFscz9tc29ja2lkPTMwNzJiYjk4Yjg0OTY0MTQzMDgxYWY0OWI5YzI2NWM3&amp;ntb=1\" target=\"_blank\"><div class=\"tpic\"><div class=\"wr_fav\" data-priority=\"2\"><div class=\"cico siteicon\" style=\"width:32px;height:32px;\"><div class=\"rms_iac\" data-alt=\"Global web icon\" data-class=\"rms_img\" data-height=\"32\" data-src=\"//th.bing.com/th?id=ODLS.A2450BEC-5595-40BA-9F13-D9EC6AB74B9F&amp;w=32&amp;h=32&amp;qlt=99&amp;pcl=fffffa&amp;o=6&amp;pid=1.2\" data-width=\"32\" style=\"height:32px;line-height:32px;width:32px;\"></div></div></div></div><div class=\"tptxt\"><div class=\"tptt\">coursera.org</div><div class=\"tpmeta\"><div class=\"b_attribution\" tabindex=\"0\" u=\"9|5108|4897238110122141|vXOZuHiqJR2zAdMSPd7qfv2MLmgUVFzP\"><cite>https://<strong>www.coursera.org</strong>/learn/<strong>programming-fundamentals</strong></cite><span class=\"c_tlbxTrg\"><span class=\"c_tlbxH\" h=\"BASE:CACHEDPAGEDEFAULT\" k=\"SERP,5289.1\"></span></span></div></div></div></a>,\n",
       " <a h=\"ID=SERP,5288.2\" href=\"https://www.bing.com/ck/a?!&amp;&amp;p=a188e7180426a08cJmltdHM9MTcyMjcyOTYwMCZpZ3VpZD0zMDcyYmI5OC1iODQ5LTY0MTQtMzA4MS1hZjQ5YjljMjY1YzcmaW5zaWQ9NTI4OA&amp;ptn=3&amp;ver=2&amp;hsh=3&amp;fclid=3072bb98-b849-6414-3081-af49b9c265c7&amp;u=a1aHR0cHM6Ly93d3cuY291cnNlcmEub3JnL2xlYXJuL3Byb2dyYW1taW5nLWZ1bmRhbWVudGFscz9tc29ja2lkPTMwNzJiYjk4Yjg0OTY0MTQzMDgxYWY0OWI5YzI2NWM3&amp;ntb=1\" target=\"_blank\"><strong>Programming Fundamentals</strong> | <strong>Coursera</strong></a>,\n",
       " <a class=\"sb_inactP sb_pagP sb_pagP_bp b_widePag sb_bp b_roths\"></a>,\n",
       " <a aria-label=\"Page 1\" class=\"sb_pagS sb_pagS_bp b_widePag sb_bp sb_pag_first\">1</a>,\n",
       " <a aria-label=\"Page 2\" class=\"b_widePag sb_bp\" h=\"ID=SERP,5338.1\" href=\"/search?q=programming&amp;FPIG=92134EEB9CE74D87B3267517210F4A3B&amp;first=11&amp;FORM=PERE\">2</a>,\n",
       " <a aria-label=\"Page 3\" class=\"b_widePag sb_bp\" h=\"ID=SERP,5339.1\" href=\"/search?q=programming&amp;FPIG=92134EEB9CE74D87B3267517210F4A3B&amp;first=21&amp;FORM=PERE1\">3</a>,\n",
       " <a aria-label=\"Page 4\" class=\"b_widePag sb_bp\" h=\"ID=SERP,5340.1\" href=\"/search?q=programming&amp;FPIG=92134EEB9CE74D87B3267517210F4A3B&amp;first=31&amp;FORM=PERE2\">4</a>,\n",
       " <a aria-label=\"Page 5\" class=\"b_widePag sb_bp\" h=\"ID=SERP,5341.1\" href=\"/search?q=programming&amp;FPIG=92134EEB9CE74D87B3267517210F4A3B&amp;first=41&amp;FORM=PERE3\">5</a>,\n",
       " <a aria-label=\"Next page\" class=\"sb_pagN sb_pagN_bp b_widePag sb_bp\" h=\"ID=SERP,5342.1\" href=\"/search?q=programming&amp;FPIG=92134EEB9CE74D87B3267517210F4A3B&amp;first=11&amp;FORM=PORE\" title=\"Next page\"><div class=\"sw_next\">Next</div></a>,\n",
       " <a aria-label=\"Search more\" class=\"mfa_btn\" h=\"ID=SERP,5365.1\" href=\"javascript:void(0);\" id=\"mfa_srch\" role=\"button\"><div class=\"fba_sbicn\" style=\"width: 40px; height: 40px;\"></div></a>,\n",
       " <a class=\"b_toggle\" h=\"ID=SERP,5367.1\" href=\"/search?q=programming&amp;FORM=000017&amp;qpvt=programming\" role=\"menuitem\">All</a>,\n",
       " <a class=\"b_toggle\" h=\"ID=SERP,5368.1\" href=\"/search?q=programming&amp;filters=ex1%3a%22ez1%22&amp;FORM=000017\" role=\"menuitem\" tabindex=\"-1\" target=\"\">Past 24 hours</a>,\n",
       " <a class=\"b_toggle\" h=\"ID=SERP,5369.1\" href=\"/search?q=programming&amp;filters=ex1%3a%22ez2%22&amp;FORM=000017\" role=\"menuitem\" tabindex=\"-1\" target=\"\">Past week</a>,\n",
       " <a class=\"b_toggle\" h=\"ID=SERP,5370.1\" href=\"/search?q=programming&amp;filters=ex1%3a%22ez3%22&amp;FORM=000017\" role=\"menuitem\" tabindex=\"-1\" target=\"\">Past month</a>,\n",
       " <a class=\"b_toggle\" h=\"ID=SERP,5371.1\" href=\"/search?q=programming&amp;filters=ex1%3a%22ez5_19574_19939%22&amp;FORM=000017\" role=\"menuitem\" tabindex=\"-1\" target=\"\">Past year</a>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
